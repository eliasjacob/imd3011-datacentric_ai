{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Logistics: A Few Things You Should Know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Questions\n",
    "Please let me know how things are progressing for you throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "**Office hours**: If you need to meet, please [email me](elias.jacob@ufrn.br)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Requirements for This Course\n",
    "To succeed in this course, you should have:\n",
    "\n",
    "- Access to the dataset and the code provided.\n",
    "- A foundational understanding of machine learning.\n",
    "- Basic knowledge of natural language processing (NLP).\n",
    "- Proficiency in Python.\n",
    "- Familiarity with Jupyter Notebooks.\n",
    "- Basic knowledge of statistics.\n",
    "- An understanding of data science and machine learning concepts and stacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Approach\n",
    "#### Top-Down Method\n",
    "I will be employing a *top-down* teaching method, which contrasts the traditional *bottom-up* approach. In a *bottom-up* approach, you typically learn all the individual components first and then gradually combine them into more complex structures. This approach often leads to students losing motivation, lacking a sense of the \"big picture,\" and not knowing what they will need in practice.\n",
    "\n",
    "According to Harvard Professor David Perkins in his book [Making Learning Whole](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719), effective learning can be likened to learning a sport like baseball. Children are not required to memorize all the rules and understand every technical detail before they start playing. Instead, they begin by playing with a general understanding and gradually learn more rules and details over time.\n",
    "\n",
    "#### Focus on Functionality\n",
    "At the beginning of this course, prioritize understanding what things **do**, not necessarily what they **are**. You will encounter some \"black boxes\", that is, concepts or tools that we use without fully explaining them upfront. Later, we will get into the lower-level details.\n",
    "\n",
    "#### Learning by Doing and Explaining\n",
    "Research indicates that people learn best by:\n",
    "1. **Doing**: Engaging in coding and building projects.\n",
    "2. **Explaining**: Articulating what they've learned, either by writing or helping others.\n",
    "\n",
    "I will guide you through building projects and encourage you to explain these concepts to your peers.\n",
    "\n",
    "#### Learning as a Team Sport\n",
    "Studies show that teamwork significantly enhances learning. Therefore, I encourage you to:\n",
    "- Ask questions.\n",
    "- Answer questions from fellow students.\n",
    "- Collaborate on building projects.\n",
    "\n",
    "If you receive a request for help, consider it an opportunity to solidify your understanding by teaching others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Materials\n",
    "All course materials can be accessed at the course's GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Final Project\n",
    "\n",
    "Your final project will be evaluated based on:\n",
    "\n",
    "- **Technical Quality:** Robustness and efficacy of your implementation.\n",
    "- **Creativity:** Originality of your approach.\n",
    "- **Usefulness:** Practical applicability.\n",
    "- **Presentation:** Effectiveness of your project showcase.\n",
    "- **Documentation:** Clarity and thoroughness of your report.\n",
    "\n",
    "### Project Guidelines\n",
    "\n",
    "- **Individual Work:** The project must be completed individually.\n",
    "- **Submission:** Provide a link to a GitHub repository or shared folder with your code, data, and report. Use virtual environments along with a `requirements.txt` file to ensure reproducibility.\n",
    "- **Deadline:** Refer to the syllabus.\n",
    "- **Presentation:** Prepare a 10-minute presentation to demonstrate your project.\n",
    "- **Submission Platform:** Use the designated platform (e.g., SIGAA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Introduction to Data-Centric AI and Weakly Supervised Learning\n",
    "## IMD3011 - Datacentric AI\n",
    "### [Dr. Elias Jacob de Menezes Neto](https://docente.ufrn.br/elias.jacob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "- **Data-Centric Shift:**  \n",
    "  Modern AI development increasingly emphasizes the importance of data quality and scale over constant modifications of model architectures. High-quality, well-prepared datasets are foundational to achieving reliable AI performance.\n",
    "\n",
    "- **Programmatic Labeling:**  \n",
    "  Moving away from purely manual annotation, programmatic methods, including automated and heuristic labeling, provide expandable, cost-efficient, and adaptable solutions. These methods address real-world challenges like rapidly evolving data and limited expert availability.\n",
    "\n",
    "- **Subject Matter Expertise (SME):**  \n",
    "  Integrating SMEs in the AI development process is essential. Their domain knowledge refines data interpretation, feature engineering, and model evaluation, ensuring that model outputs are both accurate and practically relevant.\n",
    "\n",
    "- **Weak Supervision Types:**  \n",
    "  Weak supervision can be categorized into:\n",
    "  - **Incomplete Supervision:** Only a subset of data is labeled.\n",
    "  - **Inexact Supervision:** Labels are imprecise or aggregated (e.g., multiple instance learning).\n",
    "  - **Inaccurate Supervision:** Labeled data contains noise and errors.\n",
    "  \n",
    "- **Label Aggregation and Trade-offs:**  \n",
    "  Although weak supervision may require a larger dataset (often about twice the number of weakly labeled samples compared to fully supervised ones), it can achieve comparable performance by employing aggregation techniques and adjusting for uncertainty through probabilistic labels.\n",
    "\n",
    "- **Asymptotic Scaling and Model Robustness:**  \n",
    "  The generalization error in weak supervision decreases at a rate proportional to n^(-1/2), mirroring traditional supervised learning. Efficient use of unlabeled data and sophisticated aggregation methods further enhance robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this section, students will be able to:\n",
    "\n",
    "1) Define and differentiate between model-centric and data-centric approaches to artificial intelligence development.\n",
    "\n",
    "2) Explain the rationale behind the shift towards data-centric AI, emphasizing the importance of data quality and scale for achieving performance improvements.\n",
    "\n",
    "3) Describe how the evolution of the GPT series exemplifies the principles of data-centric AI and the impact of increasing data quantity and quality.\n",
    "\n",
    "4) Articulate the role of prompt engineering in data-centric AI and explain its significance when working with large, pre-trained models.\n",
    "\n",
    "5) Discuss the consequences of adopting a data-centric approach on various aspects of AI development, such as data collection, model training strategies, and deployment workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Centric AI: Shifting Focus from Models to Data\n",
    "\n",
    "This section discusses the recent change in artificial intelligence research, where the emphasis is increasingly on the quality and scale of data rather than just on the design of model architectures.\n",
    "\n",
    "### Model-Centric vs. Data-Centric Approaches\n",
    "\n",
    "Consider the formula:\n",
    "\n",
    "$$\n",
    "\\text{AI} = \\text{Code} + \\text{Data}\n",
    "$$\n",
    "\n",
    "This expression highlights that both the algorithms (code) and the training data have important roles in achieving success with AI.\n",
    "\n",
    "#### Conventional Model-Centric Approach\n",
    "\n",
    "- **Focus on Algorithms:**  \n",
    "  Traditionally, efforts in AI research have prioritized the development and improvement of model architectures. Researchers have invested significant time in creating complex neural networks and refining training techniques.\n",
    "\n",
    "- **Limited Attention to Data Quality:**  \n",
    "  While data is always a component, its role has often been viewed as secondary to the design of advanced models.\n",
    "\n",
    "#### Emerging Data-Centric Approach\n",
    "\n",
    "- **Emphasis on High-Quality Data:**  \n",
    "  The data-centric perspective argues that substantial improvements in AI performance mainly come from the availability of large and high-quality datasets. As a result, even modest changes in how data is handled can lead to noticeable enhancements in AI outcomes.\n",
    "\n",
    "- **Simplicity in Model Architechture After a Certain Point:**  \n",
    "  With powerful models that can learn effectively from data, the task of adapting solutions to various problems may only require altering the input data (often called prompt engineering) rather than changing the network design. In other words, once the model reaches a certain level of capacity, the data fed during both training and inference becomes the primary tool for achieving specific tasks.\n",
    "\n",
    "### The GPT Series as an Example\n",
    "\n",
    "The evolution of the GPT series, developed by OpenAI, illustrates these ideas:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/gpt_evolution.png\" width=\"70%\" height=\"70%\">\n",
    "</div>\n",
    "\n",
    "- **Left Side of the Figure:**  \n",
    "  This side of the figure shows that improvements in GPT models' performance are largely due to the increase in the amount and quality of the training data. While the architecture has been refined, it has remained mostly similar in design, with the main change being the increase in the number of parameters.\n",
    "\n",
    "- **Right Side of the Figure:**  \n",
    "  When the model is sufficiently large and well-trained, it requires only minor adjustments to the input prompts (inference data) to perform a diverse range of tasks. This means that after the initial heavy lifting during training, the model remains fixed while the data used in inference drives its functionality.\n",
    "\n",
    "### Important Points to Consider\n",
    "\n",
    "> **Key Takeaways:**\n",
    "> - **Data plays a central role** in the success of modern AI systems.\n",
    "> - **High-quality and large-scale data** have contributed more to performance gains than radical changes in model architecture alone.\n",
    "> - **Prompt engineering** becomes a useful tool when working with models that are already powerful.\n",
    "\n",
    "### Addressing Potential Questions\n",
    "\n",
    "1. **Why shift focus from models to data?**  \n",
    "   The shift is driven by the realization that even with similar model architectures, significant performance improvements are achieved by using better and more abundant training data. This approach also simplifies the work required during deployment by relying on prompt engineering rather than constantly redesigning the models.\n",
    "\n",
    "2. **What are the consequences for AI development?**  \n",
    "   Emphasizing data quality may lead to increased investment in data collection, cleaning, and augmentation. Researchers and engineers might allocate resources more toward ensuring that training sets are diverse, representative, and of high quality.\n",
    "\n",
    "3. **How does this change affect model training?**  \n",
    "   Large-scale and high-quality datasets allow models to learn more from the data itself. Once a model reaches a certain level of capability, minor modifications to input data can tailor its performance across various tasks without altering the basic architecture.\n",
    "\n",
    "### Visualizing the Concept\n",
    "\n",
    "The image below reinforces the discussion by comparing the traditional focus on model improvement with the modern emphasis on data quality:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/model_vs_datacentric.jpeg\" width=\"70%\" height=\"70%\">\n",
    "</div>\n",
    "\n",
    "The accompanying figure from [this paper](http://arxiv.org/abs/2303.10158) demonstrates that even as model complexity increases, the primary factor behind performance improvements is the scale and quality of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Principles of Data-Centric AI\n",
    "\n",
    "### Principle 1: It Centers Around Data\n",
    "1. **Data as the Primary Driver**\n",
    "\n",
    "    - The adage \"Garbage In, Garbage Out\" (GIGO) is particularly relevant in AI, emphasizing the critical role of data quality.\n",
    "    - Recent AI breakthroughs are largely attributed to the quality and quantity of data, rather than model architecture improvements.\n",
    "2. **The Power of Large Language Models**\n",
    "    - With the advent of powerful models like GPT-3, the focus has shifted from model design to prompt engineering.\n",
    "    - These models demonstrate that with sufficient scale and data, many complex tasks can be solved through clever prompting rather than architectural changes.\n",
    "\n",
    "3. **Data Hunger of Deep Learning Models**\n",
    "    - Modern deep learning models require vast amounts of data to achieve high performance.\n",
    "    - This data dependency has led to a shift in focus towards data acquisition, curation, and management.\n",
    "\n",
    "4. **The \"Solved\" Problem of Deep Learning Architectures**\n",
    "    - Many researchers argue that the fundamental challenges in designing deep learning architectures have been largely addressed.\n",
    "    - See the image below: over the last years the [performance on IMDB sentiment analysis](https://paperswithcode.com/sota/sentiment-analysis-on-imdb) has been increasing very modestly . After 5 years and millions of dollars invested, the accuracy has increased from 96.0% to 96.68%\n",
    "    - The emphasis now is on applying and fine-tuning existing architectures rather than creating entirely new ones.\n",
    "    - The \"problem\" with designing deep learning architectures is \"solved\". Today, you can load a pre-trained model with few lines of code and get latest results. The real challenge is to get the right data to train your model.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/imdb_sentiment_analysis.png\" width=\"70%\" height=\"70%\">\n",
    "</div>\n",
    "\n",
    "> <div style=\"text-align: center;\">\n",
    "> <video width=\"720\" controls>\n",
    "> <source src=\"images/febraban1.mp4\" type=\"video/mp4\">\n",
    "> </video>\n",
    ">\n",
    "\n",
    "\n",
    "Maybe a little too much? Let's see.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "````\n",
    "\n",
    "Maybe he was right, after all. 😅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle 2: It Needs to be Programmatic\n",
    "\n",
    "### Challenges with Manual Data Labeling\n",
    "\n",
    "Manual data labeling, while traditional, has several difficulties that affect the development of AI systems:\n",
    "\n",
    "- **Scalability:**  \n",
    "  As dataset sizes increase, the time required for manual labeling grows approximately as  \n",
    "  $$\n",
    "  T = N \\times t_{\\text{label}}\n",
    "  $$\n",
    "  where $ N $ is the number of samples and $ t_{\\text{label}} $ is the time spent per sample. With large $ N $, manual labeling becomes impractical.\n",
    "\n",
    "- **Cost Inefficiency:**  \n",
    "  Hiring and managing sizeable teams of human labelers can demand extensive resources. This cost can limit projects, especially when frequent dataset updates are necessary.\n",
    "\n",
    "- **Time Constraints:**  \n",
    "  The entire manual process demands significant time investment. This delay slows the overall cycle of AI model development and updates.\n",
    "\n",
    "### Specific Real-World Limitations\n",
    "\n",
    "Manual labeling presents added challenges in certain domains:\n",
    "\n",
    "- **Data Privacy:**  \n",
    "  In areas like healthcare or law, the data may include sensitive personal information that cannot be easily shared with human labelers. This restriction complicates the process and may require additional safeguards.\n",
    "\n",
    "- **Specialized Expertise:**  \n",
    "  Some projects require labelers with specific domain knowledge. For instance:\n",
    "  \n",
    "  - Legal document classification might require insights from lawyers.\n",
    "  - Medical data annotation often needs input from qualified physicians.\n",
    "  - Technical content might demand experts in the field.\n",
    "\n",
    "> **Example:**  \n",
    "> Developing an AI system to diagnose rare diseases would necessitate input from medical specialists for each case. This makes manual labeling not only slow and expensive but also subject to strict data privacy protocols.\n",
    "\n",
    "### The Issue of Evolving Information\n",
    "\n",
    "Manual labeling can also struggle with keeping pace with changes in information:\n",
    "\n",
    "- **Outdated Labels:**  \n",
    "  Fields such as law, medicine, or technology may experience rapid changes. As new laws, discoveries, or technologies emerge, previously labeled data may become less accurate or entirely irrelevant.\n",
    "\n",
    "- **Need for Continuous Updates:**  \n",
    "  When new categories must be added or existing ones updated, large parts of the dataset may need re-labeling.  \n",
    "  > **Analogy:**  \n",
    "  > This is like printing an encyclopedia: by the time it is complete, some information is already outdated, requiring a complete revision to stay current.\n",
    "\n",
    "### Moving Toward Programmatic Solutions\n",
    "\n",
    "To overcome these challenges, the AI community is increasingly using programmatic methods for data labeling:\n",
    "\n",
    "1. **Automated Labeling:**  \n",
    "   Developing algorithms that can assign labels with minimal human oversight. This method reduces both the cost and time required.\n",
    "\n",
    "2. **Transfer Learning:**  \n",
    "   Using models pre-trained on large datasets to reduce the volume of new labeled data needed for specific tasks.\n",
    "\n",
    "3. **Active Learning:**  \n",
    "   Carrying Out systems that identify and select the most informative samples for human review. This approach minimizes the overall labeling effort while maximizing learning efficiency.\n",
    "\n",
    "4. **Synthetic Data Generation:**  \n",
    "   Creating artificial data that mimics real-world properties can help increase actual datasets, addressing limitations in data quantity and diversity.\n",
    "\n",
    "### Benefits of Programmatic Approaches\n",
    "\n",
    "These automated methods offer several clear advantages:\n",
    "\n",
    "- **Speed:**  \n",
    "  Labeling can be performed much faster than with manual methods.\n",
    "  \n",
    "- **Cost:**  \n",
    "  Reduced dependency on large labeling teams leads to lower overall expenses.\n",
    "\n",
    "- **Adaptability:**  \n",
    "  Automated systems can be updated quickly as new information becomes available, ensuring the labeling process remains current.\n",
    "\n",
    "- **Scalability:**  \n",
    "  Programmatic labeling scales efficiently with data volume, accommodating rapid growth in dataset size.\n",
    "\n",
    "<br><br>\n",
    "| Labeling Approach      | Speed | Cost      | Adaptability |\n",
    "|------------------------|-------|-----------|--------------|\n",
    "| Manual Labeling        | Slow  | Expensive | Static       |\n",
    "| Programmatic Labeling  | Fast  | Affordable| Dynamic      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle 3: It Needs to Include Subject Matter Expertise in the Loop\n",
    "\n",
    "### The Importance of Subject Matter Experts (SMEs)\n",
    "\n",
    "In many machine learning projects, the input of Subject Matter Experts (SMEs) is essential. Their specific knowledge and insights bring an understanding of the domain that can greatly improve the accuracy and applicability of AI systems.\n",
    "\n",
    "- **Critical Contributions:**  \n",
    "  SMEs offer context that extends beyond data and algorithms. They can clarify ambiguous data points, provide guidance on potential pitfalls, and help align model outputs with real-world requirements.\n",
    "\n",
    "- **Team Incorporation:**  \n",
    "  Rather than being seen as external consultants, SMEs should be regarded as key members of the project team. Their continuous participation shapes decisions on model design, data interpretation, and feature engineering.\n",
    "\n",
    "### Effective Collaboration with SMEs\n",
    "\n",
    "Working effectively with SMEs involves clear communication and mutual respect:\n",
    "\n",
    "- **Clear Communication:**  \n",
    "  - Translate technical terms into language that is easily understood by experts in the domain.  \n",
    "  - Listen carefully to their feedback and use it to refine model objectives.  \n",
    "  - Ask specific questions to uncover hidden assumptions in the data or problem setup.\n",
    "\n",
    "- **Regular Interaction:**  \n",
    "  Frequent discussions throughout the project help ensure that the insights from domain experts remain aligned with the model's development. For example, scheduling regular check-ins helps both technical and domain teams stay informed about ongoing adjustments.\n",
    "\n",
    "### Navigating Domain Complexity\n",
    "\n",
    "Machine learning practitioners often work across a variety of fields, which may include healthcare, finance, legal, or technology, among others. Consider the following points:\n",
    "\n",
    "- **Recognize Limits:**  \n",
    "  It is unrealistic to be an expert in every field. Acknowledging this fact encourages a learning mindset and emphasizes the value of expert input.\n",
    "\n",
    "- **Continuous Learning:**  \n",
    "  Every project serves as an opportunity to learn more about a new domain. Over time, the process of incorporating SME insights into technical models enhances your capability to adapt to various subjects.\n",
    "\n",
    "- **Encoding Knowledge:**  \n",
    "  The challenge is to incorporate domain knowledge into your models effectively. This may involve using established domain theories to guide data preprocessing, feature selection, and model evaluation. In some cases, the SME contributions can be represented mathematically. For example, if a feature $ x_i $ is informed by domain insights, one can express the importance of that feature as a weight $ w_i $ in the following linear model:\n",
    "  $$\n",
    "  y = \\sum_{i=1}^{n} w_i x_i + b\n",
    "  $$\n",
    "  where the weights $ w_i $ are tuned not only through standard optimization methods but also adjusted based on SME feedback.\n",
    "\n",
    "### Strategies for Integrating SME Insights\n",
    "\n",
    "To maximize the benefits of SME participation, consider the following approaches:\n",
    "\n",
    "1. **Frequent Check-Ins:**  \n",
    "   Schedule regular meetings with SMEs throughout the project lifecycle. This ongoing interaction helps address issues as they arise.\n",
    "\n",
    "2. **Collaborative Problem Definition:**  \n",
    "   Involve SMEs in determining project aims and defining success criteria. Their input ensures that the project goals are both realistic and relevant to the domain.\n",
    "\n",
    "3. **Data Interpretation:**  \n",
    "   Use SME insights to help interpret data, especially when dealing with subtle or ambiguous statistical patterns. Their understanding can clarify why certain anomalies arise.\n",
    "\n",
    "4. **Feature Engineering:**  \n",
    "   Work together to select or create features that capture domain-specific nuances. This might involve identifying key indicators or combining variables in a meaningful way.\n",
    "\n",
    "5. **Model Evaluation:**  \n",
    "   Go beyond common performance metrics by incorporating SME observations into the evaluation process. Their feedback can reveal performance issues that numbers alone might not show.\n",
    "\n",
    "6. **Iterative Refinement:**  \n",
    "   As the project evolves, continue to refine the model based on ongoing SME input. This iterative process helps align technical outcomes with practical expectations.\n",
    "\n",
    "### Addressing Common Challenges\n",
    "\n",
    "Despite the benefits, integrating SME feedback can come with challenges:\n",
    "\n",
    "- **Bridging Knowledge Gaps:**  \n",
    "  Form strategies that ease better communication between technical teams and domain experts. Clear, jargon-free dialogue is key.\n",
    "\n",
    "- **Managing Expectations:**  \n",
    "  Clearly state what machine learning can and cannot achieve. This helps align SME expectations with the model’s capabilities.\n",
    "\n",
    "- **Balancing Technical and Domain Perspectives:**  \n",
    "  Find a middle ground where technical feasibility and domain-specific requirements meet. Align both perspectives to achieve solutions that work well statistically and practically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data-Centric Approach\n",
    "\n",
    "> \"Data is both the key bottleneck and interface to developing AI today\"\n",
    "\n",
    "This statement highlights two important ideas:\n",
    "\n",
    "- **Data as a Bottleneck**: Often, the main limitation in advancing AI is not the computational power or the model's design but rather the availability of high-quality, relevant data.\n",
    "- **Data as an Interface**: Data acts as the key element through which we shape and interact with AI systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Strategies for Data-Centric AI\n",
    "\n",
    "To enhance AI development by focusing on data, consider the following strategies:\n",
    "\n",
    "1. **Focus on Data Quality**\n",
    "   - Place emphasis on collecting and curating data that is both accurate and relevant.\n",
    "   - Establish and follow clear procedures for data validation and verification.\n",
    "   - **Example:** Imagine training a predictive model. The performance of the model depends on how well the training data represents real-world conditions, which is similar to how clear and precise measurements in an experiment lead to more reliable results.\n",
    "\n",
    "2. **Data Augmentation and Synthesis**\n",
    "   - Use techniques to expand existing datasets without compromising quality.\n",
    "   - Explore methods to generate synthetic data, especially when data is limited in particular areas.\n",
    "   - **Mathematical View:** Let $ D $ represent the available data. Augmentation aims to create an enhanced dataset $ D' $ such that:\n",
    "     $$\n",
    "     D' = D \\cup A(D)\n",
    "     $$\n",
    "     where $ A(D) $ represents the augmented data generated from the original dataset.\n",
    "\n",
    "3. **Efficient Data Labeling**\n",
    "   - Consider methods like weak supervision and semi-supervised learning to minimize the need for extensive manual labeling.\n",
    "   - Develop or use tools that assist in the labeling process by incorporating predictions from existing models.\n",
    "   - **Analogy:** Think of labeling as tagging books in a library; efficient systems reduce the overall workload while still ensuring that every book is properly categorized.\n",
    "\n",
    "4. **Data Governance and Ethics**\n",
    "   - Create clear guidelines for how data is collected, used, and stored.\n",
    "   - Address concerns such as data bias, ensuring that data collection covers a fair and diverse sample, and safeguard privacy.\n",
    "   - **Note:** Integrating ethics into data management helps prevent unintended consequences in AI applications.\n",
    "\n",
    "5. **Continuous Data Improvement**\n",
    "   - Establish processes for regular updates and improvements to the dataset.\n",
    "   - Develop metrics and techniques for ongoing assessment of data quality.\n",
    "   - **Example:** Similar to how students improve their learning materials with feedback over a term, the data used in AI can be continually refined to improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Challenges and Considerations\n",
    "\n",
    "Several challenges may arise when adopting a data-centric approach:\n",
    "\n",
    "- **Balancing Quantity and Quality**\n",
    "  - While adding more data can be beneficial, it is essential to ensure that new data maintains or improves the overall quality.\n",
    "  - **Question to Consider:** How do we ensure that increased data volume does not compromise accuracy?\n",
    "\n",
    "- **Domain Specificity**\n",
    "  - The benefits of a data-focused strategy may vary depending on the application area. Some domains might require specialized data collection and curation methods.\n",
    "  - **Clarification:** Different fields may have unique data requirements. For instance, medical data demands strict privacy and high accuracy, while social media data might prioritize volume and trend extraction.\n",
    "\n",
    "- **Computational Resources**\n",
    "  - Managing and processing large datasets requires considerable computational power.\n",
    "  - **Consideration:** Ensure that the available infrastructure can support the data operations needed.\n",
    "\n",
    "- **Interdisciplinary Collaboration**\n",
    "  - Effective data management in AI often calls for the combined efforts of data scientists, domain experts, and AI researchers.\n",
    "  - **Example:** In environmental modeling, collaboration between meteorologists and data experts can lead to better data collection practices, ensuring that models more accurately reflect climate patterns.\n",
    "\n",
    "> **Important:** Shifting to a data-centric approach in AI emphasizes the importance of data quality and relevance. Often, improvements in data can lead to more significant gains in model performance than changes in model architecture. This change in focus can lead to more stable and reliable AI systems across various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Don't Have Enough Data! What Can I Do?\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/insufficient_labeled_training_data.png\" width=\"70%\" height=\"70%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### 1. Expert Hand-Labeling\n",
    "\n",
    "**Overview:**  \n",
    "- Involves subject matter experts (SMEs) manually annotating data.\n",
    "- **Traditional Supervision:** Randomly label data points without a specific strategy.\n",
    "- **Active Learning:** Select the most informative or uncertain data points first to reduce labeling efforts.\n",
    "\n",
    "**Considerations:**\n",
    "- **Accuracy:** High-quality labels benefiting from deep domain knowledge.\n",
    "- **Cost and Time:** Manual processes can be slow and expensive.\n",
    "- **Scalability:** Not well-suited for extremely large datasets.\n",
    "\n",
    "**Key Points:**\n",
    "- When using expert labeling, imagine your labeled data as a set of indices:  \n",
    "  $$\n",
    "  D_{\\text{labeled}} = \\{(x_i, y_i) \\}_{i=1}^{n}\n",
    "  $$\n",
    "  Here, $n$ is often small due to resource constraints.\n",
    "- Active learning strategies seek to maximize the utility of each labeled data point.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Weak Supervision\n",
    "\n",
    "**Overview:**  \n",
    "Weak supervision refers to techniques that generate labels with lower precision but at significantly reduced cost and effort.\n",
    "\n",
    "**Methods:**\n",
    "- **Programmatic Supervision:**  \n",
    "  - Develop rules, patterns, or heuristics to label data automatically.\n",
    "  - Rules can be defined as functions $ f: X \\to Y $ that map features to approximate labels.\n",
    "- **Crowdsourcing:**  \n",
    "  - Distribute labeling tasks among many contributors.\n",
    "  - Often involves aggregating responses from multiple non-expert labelers to approximate expert quality.\n",
    "- **Heuristic Supervision:**  \n",
    "  - Use existing metadata or domain-specific rules to assign labels without manual review of every sample.\n",
    "\n",
    "**Considerations:**\n",
    "- **Trade-Off:** Lower per-instance accuracy may be offset by scale and speed.\n",
    "- **Aggregate Reliability:** Techniques such as majority voting can help improve overall label quality.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Semi-Supervised Learning\n",
    "\n",
    "**Overview:**  \n",
    "Semi-supervised learning is effective when you have a small set of labeled data combined with a much larger pool of unlabeled data.\n",
    "\n",
    "**Techniques:**\n",
    "- **Utilizing Unlabeled Data:**  \n",
    "  - Incorporate unlabeled data, $D_{\\text{unlabeled}} = \\{ x_j \\}_{j=1}^{m}$, to learn structure or patterns in the data.\n",
    "  - Combine with the labeled set to improve overall model performance.\n",
    "- **Loss Function Incorporation:**  \n",
    "  - Training often involves a combined loss function that includes both supervised and unsupervised components:\n",
    "    $$\n",
    "    \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{supervised}} + \\lambda \\, \\mathcal{L}_{\\text{unsupervised}}\n",
    "    $$\n",
    "    where $\\lambda$ controls the influence of the unlabeled data.\n",
    "\n",
    "**Considerations:**\n",
    "- **Generalization:** Using unlabeled data affords the model additional context, potentially reducing overfitting on the small labeled subset.\n",
    "- **Implementation:** Requires algorithms designed to extract useful features from unlabeled examples, such as consistency regularization or pseudo-labeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Transfer Learning\n",
    "\n",
    "**Overview:**  \n",
    "Transfer learning involves taking a model trained on one task and adapting it to another task with limited labeled data.\n",
    "\n",
    "**Methods:**\n",
    "- **Pre-training and Fine-tuning:**  \n",
    "  - Begin with a model pre-trained on a related dataset.\n",
    "  - Adapt the model to your target domain by fine-tuning on your specific (and typically small) labeled dataset.\n",
    "- **Multi-Task Learning:**  \n",
    "  - Train models on multiple related tasks simultaneously.\n",
    "  - Share learned representations across tasks, which can improve performance when target data is scarce.\n",
    "- **Using Models Trained on Similar Domains:**  \n",
    "  - If available, use models that have been developed on analogous problems.\n",
    "  - Even if the data quality varies, these models can provide a useful starting point.\n",
    "\n",
    "**Considerations:**\n",
    "- **Domain Similarity:**  \n",
    "  - The success of transfer learning depends on how similar the source and target domains are.\n",
    "- **Data Efficiency:**  \n",
    "  - Transfer learning can be especially powerful when labeled data is very limited, effectively reducing the requirement for large amounts of task-specific example data.\n",
    "\n",
    "---\n",
    "\n",
    "### Strategic Considerations\n",
    "\n",
    "When deciding on a strategy for addressing insufficient labeled training data, consider the following factors:\n",
    "\n",
    "- **Resources Available:**  \n",
    "  - Time, labor, expertise, and computing resources.\n",
    "- **Domain Complexity:**  \n",
    "  - The intricacy of the subject matter may dictate a need for expert labels.\n",
    "- **Precision Requirements:**  \n",
    "  - Critical applications might demand higher quality labels.\n",
    "- **Computational Capabilities:**  \n",
    "  - More sophisticated methods like semi-supervised or transfer learning might require additional computing power.\n",
    "\n",
    "**Recommendation:**\n",
    "- Often, combining multiple strategies will yield the best results. For example, expert hand-labeling on a key subset of the data can be combined with weak supervision to label the remaining data, and semi-supervised learning can further capitalize on any unlabeled examples.\n",
    "\n",
    "> **Note:**  \n",
    "> There is no one-size-fits-all approach to data scarcity. The effectiveness of each method will vary depending on the context. It is important to assess your data quality and continuously refine your approach in light of model performance and emerging insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Supervised to Weakly Supervised Learning\n",
    "\n",
    "### Fully Supervised Learning\n",
    "\n",
    "Fully supervised learning is a basic method in machine learning that trains a model using a dataset where every example has an associated label. The goal is to learn a mapping from inputs to outputs, which can be applied to new, unseen data.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Function Mapping**\n",
    "\n",
    "   - **Definition:** Learn a function $ f: \\mathcal{X} \\rightarrow \\mathcal{Y} $.\n",
    "     - $ \\mathcal{X} $ is the input space (e.g., images, text, or sensor readings).\n",
    "     - $ \\mathcal{Y} $ is the output space (e.g., categories, binary values, or continuous numbers).\n",
    "   - **Example:** In image classification, $ \\mathcal{X} $ represents images while $ \\mathcal{Y} $ represents a set of labels such as \"cat\" or \"dog\".\n",
    "\n",
    "2. **Dataset Structure**\n",
    "\n",
    "   - **Training Data:** A labeled dataset denoted by\n",
    "     $$\n",
    "     \\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_m, y_m)\\}\n",
    "     $$\n",
    "     Here, each $ (x_i, y_i) $ is an individual example.\n",
    "     - $ x_i \\in \\mathcal{X} $: The input feature.\n",
    "     - $ y_i \\in \\mathcal{Y} $: The corresponding label.\n",
    "   - **Role of Data:** The dataset must be representative of the problem space to ensure the model generalizes well to unseen cases.\n",
    "\n",
    "3. **Variants of Supervised Learning Tasks**\n",
    "\n",
    "   - **Binary Classification:** $ \\mathcal{Y} = \\{\\text{Yes}, \\text{No}\\} $.\n",
    "   - **Multiclass Classification:** $ \\mathcal{Y} = \\{\\text{Class}_1, \\text{Class}_2, \\dots, \\text{Class}_n\\} $.\n",
    "   - **Regression:** $ \\mathcal{Y} = \\mathbb{R} $ (real numbers).\n",
    "\n",
    "#### The Learning Process\n",
    "\n",
    "1. **Data Collection:**  \n",
    "   Gather an extensive and representative dataset that includes both input features and corresponding labels.\n",
    "\n",
    "2. **Feature Selection:**  \n",
    "   Identify and choose relevant features from the input data that contribute to predicting the output accurately.\n",
    "\n",
    "3. **Model Selection:**  \n",
    "   Decide on an appropriate algorithm or model structure capable of capturing the relationship between inputs and outputs.\n",
    "\n",
    "4. **Training:**  \n",
    "   Improve the model’s parameters using the training data to reduce the difference between the model’s predictions and the actual labels.\n",
    "\n",
    "5. **Validation:**  \n",
    "   Evaluate the model on a separate set of data that was not used during training. This step helps to check how well the model generalizes.\n",
    "\n",
    "6. **Fine-Tuning:**  \n",
    "   Adjust parameters or modify the model structure based on performance feedback until a satisfactory level of accuracy is achieved.\n",
    "\n",
    "#### Objective Function\n",
    "\n",
    "To train the model, we minimize a loss function that measures the error between the model's predictions and the true labels. This is usually expressed as:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(f_{\\theta}(x_i), y_i)\n",
    "$$\n",
    "\n",
    "- $ \\theta $ represents the model parameters.\n",
    "- $ f_{\\theta}(x_i) $ is the prediction for the input $ x_i $.\n",
    "- $ \\mathcal{L}(f_{\\theta}(x_i), y_i) $ is the loss associated with the prediction.\n",
    "\n",
    "> **Note:** This equation means that we are trying to find the best model settings that, on average, make our predictions as close as possible to the actual labels.\n",
    "\n",
    "#### Advantages and Challenges\n",
    "\n",
    "- **Advantages:**\n",
    "  - **High Accuracy:** With a sufficient amount of accurately labeled data, supervised models can reach high levels of accuracy.\n",
    "  - **Clear Evaluation Metrics:** Since true labels are known, performance metrics are straightforward to compute (e.g., accuracy, precision, recall).\n",
    "  - **Theoretical and Practical Support:** There is a strong foundation in both theory and applied methods for fully supervised learning.\n",
    "\n",
    "- **Challenges:**\n",
    "  - **Data Requirements:** A large amount of labeled data is needed, which can be costly and time-consuming to collect.\n",
    "  - **Generalization Risk:** If the training data does not fully represent the problem space, the model might not perform well on new data.\n",
    "  - **Overfitting:** Complex models may learn the training data too well, including its noise, which can reduce performance on unseen data.\n",
    "\n",
    "> **Reminder:** Fully supervised learning is effective when there is plenty of labeled data. In situations where obtaining such data is difficult, alternative methods like weakly supervised or unsupervised learning may provide viable solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Weak Supervision\n",
    "\n",
    "> *“Weakly supervised learning is an umbrella term covering a variety of studies that attempt to construct predictive models by learning with weak supervision.”*\n",
    "\n",
    "Weak supervision in machine learning refers to scenarios where the available training labels are not as complete, precise, or error-free as in fully supervised learning. This section categorizes weak supervision into three primary types: **Incomplete Supervision**, **Inexact Supervision**, and **Inaccurate Supervision**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Incomplete Supervision\n",
    "\n",
    "<img src=\"images/incomplete_supervision.png\" width=\"70%\" height=\"70%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "**Definition:**  \n",
    "Incomplete supervision occurs when only a subset of the available training data is labeled. In many practical applications, obtaining labels for every data point can be very expensive or time-consuming. As a result, a large portion of the data remains unlabeled.\n",
    "\n",
    "**Characteristics:**\n",
    "- Only a fraction of the data points have labels.\n",
    "- The unlabeled data may be much larger in proportion compared to the labeled set.\n",
    "  \n",
    "**Formal Representation:**  \n",
    "If we denote the labeled data by $ l $ and the unlabeled data by $ u $, with the total data being $ m = l + u $, the dataset can be written as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_1, y_1), \\dots, (x_l, y_l), (x_{l+1}, \\emptyset), \\dots, (x_m, \\emptyset)\\}\n",
    "$$\n",
    "\n",
    "**Example:**  \n",
    "In a collection of medical images, only a small number may be annotated by experts due to the high cost and expertise required for labeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inexact Supervision\n",
    "\n",
    "<img src=\"images/inexact_supervision.png\" width=\"70%\" height=\"70%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "**Definition:**  \n",
    "Inexact supervision refers to situations in which the labels available do not provide the ideally detailed or instance-specific information. Instead, the labels often supply higher-level summaries or aggregated data.\n",
    "\n",
    "**Characteristics:**\n",
    "- Labels are less precise, often providing only partial or aggregate information.\n",
    "- Annotations may be available at a group level rather than for each individual instance.\n",
    "\n",
    "#### Common Forms of Inexact Supervision\n",
    "\n",
    "1. **Multiple Instance Learning (MIL):**  \n",
    "   - **Concept:** Data is grouped into \"bags\" of instances and the entire bag is labeled rather than each instance.\n",
    "   - **Mathematical Formulation:**  \n",
    "     For a bag $ B_i = \\{x_{i1}, x_{i2}, \\dots, x_{in}\\} $, the bag label $ y_i $ is defined as:\n",
    "     $$\n",
    "     y_i =\n",
    "     \\begin{cases}\n",
    "     1 & \\text{if there exists } x_{ij} \\in B_i \\text{ such that } y_{ij} = 1, \\\\\n",
    "     0 & \\text{otherwise.}\n",
    "     \\end{cases}\n",
    "     $$\n",
    "\n",
    "2. **Label Proportions:**  \n",
    "   - **Concept:** Instead of individual instance labels, the proportion of positive instances in each group is known.\n",
    "   - **Mathematical Formulation:**  \n",
    "     For a group $ G_k $ with $ n_k $ instances, the proportion is given as:\n",
    "     $$\n",
    "     p_k = \\frac{1}{n_k} \\sum_{x_i \\in G_k} y_i\n",
    "     $$\n",
    "     where $ p_k $ represents the known proportion of positive instances.\n",
    "\n",
    "3. **Coarse-Grained Labels:**  \n",
    "   - **Concept:** Labels are provided at a more general level rather than being specific to each fine-grained category.\n",
    "   - **Mathematical Formulation:**  \n",
    "     Let $ Y $ be the set of fine-grained labels, and $ Z $ be the set of coarse-grained labels such that:\n",
    "     $$\n",
    "     z = f(y), \\quad \\text{with } y \\in Y,\\, z \\in Z, \\quad \\text{and } |Z| < |Y|.\n",
    "     $$\n",
    "\n",
    "**Examples:**\n",
    "- **Image Classification:** An image might be labeled as \"cat,\" but the precise location of the cat within the image is not provided.\n",
    "- **Text Analysis:** A document might be classified as having a positive sentiment without indicating which specific parts contribute to this sentiment.\n",
    "- **Epidemiology:** Reports may provide the prevalence of a disease in a region without recording individual-level diagnoses.\n",
    "\n",
    "> **Note:** Inexact supervision is often integrated with other methods in a larger pipeline rather than used on its own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inaccurate Supervision\n",
    "\n",
    "<img src=\"images/inaccurate_supervision.png\" width=\"70%\" height=\"70%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "**Definition:**  \n",
    "Inaccurate supervision arises when the training labels contain errors or noise. Although the dataset is formally structured as in fully supervised learning, the labels $ y_i $ cannot be fully trusted due to their built-in inaccuracy.\n",
    "\n",
    "**Sources of Inaccuracy:**\n",
    "- **Human Error:** Mistakes made during manual labeling.\n",
    "- **Automated Labeling Errors:** Faulty processes in automated systems.\n",
    "- **Built-In Ambiguity:** Certain tasks are ambiguous by nature.\n",
    "- **Deliberate Noise Injection:** Sometimes noise is intentionally introduced during training.\n",
    "\n",
    "**Modeling Label Noise:**  \n",
    "A common approach is to model the noise using a transition matrix $ T $, which represents the probability of a true label being flipped to an incorrect label. The effective label can be expressed as:\n",
    "$$\n",
    "\\tilde{y} = T y\n",
    "$$\n",
    "\n",
    "#### Types of Label Noise\n",
    "\n",
    "1. **Random Noise:**\n",
    "   - **Description:** Labels are flipped or misassigned at random.\n",
    "   - **Binary Classification Example:** The noise transition matrix for binary labels might be:\n",
    "     $$\n",
    "     T = \\begin{bmatrix}\n",
    "     1-\\rho_1 & \\rho_2 \\\\\n",
    "     \\rho_1 & 1-\\rho_2\n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "     Here, $ \\rho_1 $ is the probability of flipping a label from 0 to 1 and $ \\rho_2 $ is the probability of flipping from 1 to 0.\n",
    "\n",
    "2. **Systematic Noise:**\n",
    "   - **Description:** Errors occur in a patterned or biased manner, often due to consistent misunderstandings.\n",
    "   - **Modeling Approach:**\n",
    "     $$\n",
    "     P(\\tilde{y} | y) = g(y)\n",
    "     $$\n",
    "     where $ g(y) $ represents a function that captures the error pattern based on the true label $ y $.\n",
    "\n",
    "3. **Instance-Dependent Noise:**\n",
    "   - **Description:** The mislabeling probability varies with the features of each instance.\n",
    "   - **Modeling Approach:**\n",
    "     $$\n",
    "     P(\\tilde{y} | y, x) = h(y, x)\n",
    "     $$\n",
    "     Here, $ x $ denotes the features of the instance, indicating that the likelihood of a label being noisy depends on the instance itself.\n",
    "\n",
    "**Challenges Introduced by Inaccurate Supervision:**\n",
    "- The risk of the model learning erroneous patterns from noisy labels.\n",
    "- Difficulties in distinguishing reliable patterns from noise.\n",
    "- The potential for overfitting to incorrect labels.\n",
    "- Reduced performance and generalization ability.\n",
    "\n",
    "**Research Directions:**\n",
    "- Improvement in methods to estimate the noise transition matrix $ T $.\n",
    "- Development of strong loss functions that account for label noise, for example:\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{reliable}}(\\theta) = \\mathbb{E}_{(x,\\tilde{y})}\\left[\\ell(f_\\theta(x), \\tilde{y}) \\mid T \\right]\n",
    "  $$\n",
    "- Investigations in theoretical bounds when learning with noisy data.\n",
    "- Methods in confident learning, which incorporate the model's confidence to better handle noisy labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overcoming the Unavailability of Labels: Weak Supervision\n",
    "\n",
    "Weak supervision offers a useful method to overcome the problem of limited labeled data in machine learning. It uses domain expertise and a set of rules or heuristics to generate labels for large datasets quickly and efficiently.\n",
    "\n",
    "### Key Concepts of Weak Supervision\n",
    "\n",
    "1. **Programmatic Labeling**\n",
    "   - **Heuristic Labeling Functions:** Instead of manual annotations, labels are produced by applying rules that represent domain knowledge. These rules assign labels automatically to data points based on specific criteria.\n",
    "   - **Efficiency:** This strategy allows practitioners to generate vast numbers of labels rapidly, which is especially valuable when expert human labeling is costly or time-consuming.\n",
    "\n",
    "2. **Generalization from Weak Labels**\n",
    "   - **Learning Beyond Simple Patterns:** Though the initial labels come from heuristics that tie closely to the domain-based rules, the classifiers trained on these labels can learn to capture more complex relationships within the data.\n",
    "   - **Transition from Weak to Strong:** Over time, the model may generalize well beyond the limitations of the original labeling functions.\n",
    "\n",
    "3. **Quantity vs. Quality Trade-off**\n",
    "   - **Data Volume:** Empirical results indicate that about twice as many weakly labeled samples may be needed to achieve the performance seen with manually labeled data.\n",
    "   - **Cost Benefits:** Despite the requirement for more data, the significant reduction in labeling cost justifies the approach.\n",
    "\n",
    "4. **Scalability**\n",
    "   - **Large-Scale Applications:** The method is exceptionally flexible since there is only a minimal extra cost in generating a million labels compared to a single label.\n",
    "   - **Cost Efficiency:** This efficiency makes weak supervision accessible for projects involving large datasets.\n",
    "\n",
    "### Effectiveness of Weak Supervision\n",
    "\n",
    "Experimental evidence supports the practical use of weak supervision:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/ws1.png\" width=\"70%\" height=\"70%\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/ws2.png\" width=\"70%\" height=\"70%\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/ws3.png\" width=\"70%\" height=\"70%\">\n",
    "</div>\n",
    "\n",
    "These images show that weak supervision can achieve performance similar to, or even better than, traditional supervised learning methods in various tasks.\n",
    "\n",
    "### Aggregating Noisy Labels\n",
    "\n",
    "A key challenge of weak supervision is how to combine labels from various heuristic sources, which might be noisy or partially conflicting:\n",
    "\n",
    "1. **Simple Aggregation Methods**\n",
    "   - **Majority Voting:** A basic method where the label selected by most heuristics is chosen. However, this method might miss the fundamental relationships between the different rules.\n",
    "\n",
    "2. **Advanced Aggregation Techniques**\n",
    "   - **Graph Neural Networks and Matrix Completion:** These advanced strategies better capture the dependencies among the different labeling functions, leading to more reliable aggregated labels.\n",
    "\n",
    "3. **Unsupervised Accuracy Estimation**\n",
    "   - **Estimating Performance Without Ground Truth:** It is possible to estimate the accuracy and interdependencies of heuristics even when no true labels are available. This estimation helps to flag which rules are more reliable.\n",
    "\n",
    "4. **Calibration with a Small Labeled Set**\n",
    "   - **Validation:** Typically, a small set of manually labeled observations (around 400-600) is used to calibrate the labeling functions and verify their performance.\n",
    "   - **Refinement:** This set is critical to adjust the heuristics and ensure that the resulting labels are trustworthy.\n",
    "\n",
    "5. **Probabilistic Labeling**\n",
    "   - **Confidence Estimates:** The outcome of the aggregation is a probabilistic label for each observation, which reflects the uncertainty innate in using weak labels.\n",
    "\n",
    "\n",
    "### Asymptotic Scaling Behavior\n",
    "\n",
    "The scaling behavior of weak supervision has a mathematical foundation:\n",
    "\n",
    "- **Generalization Error:** Studies, such as those by [Ratner et al](http://arxiv.org/abs/1810.02840), demonstrate that as the number of unlabeled data points $ \\mathcal{n} $ increases, the generalization error decreases following an order of $ \\mathcal{n}^{-\\frac{1}{2}} $. This rate is similar to that seen in traditional supervised learning with manually labeled data.\n",
    "- **Label Model Impact:** When unlabeled data is used in combination with a well-calibrated label model $ \\hat{\\mu} $, the performance converges to that of models trained with fully labeled datasets.\n",
    "\n",
    "This mathematical insight reassures us that practical use of weak supervision can yield reliable predictive performance when sufficient data is available.\n",
    "\n",
    "\n",
    "### Reliability of Weakly Labeled Datasets\n",
    "\n",
    "A common concern is the trustworthiness of weakly labeled datasets. It is important to recognize that human-annotated datasets also contain errors. For example, studies report the following label error rates for widely used datasets ([Northcutt et al, 2021](http://arxiv.org/abs/2103.14749)):\n",
    "\n",
    "- **CIFAR-100:** 5.85%\n",
    "- **ImageNet:** 5.83%\n",
    "- **Google QuickDraw:** 10.12%\n",
    "- **IMDB Reviews:** 2.9%\n",
    "- **Amazon Reviews:** 3.9%\n",
    "\n",
    "> **Note:** Even benchmark datasets widely accepted in the research community are not free from labeling errors, emphasizing the necessity for methods that can identify and correct such mistakes in both human and weak supervision pipelines.\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "The method's reliability is based on the observation that labeling errors are not random. Mislabeling is more common between similar classes. For example, consider the following inequality from [Angluin and Laird (1988)](https://dl.acm.org/doi/10.1023/A%3A1022873112823):\n",
    "\n",
    "$$\n",
    "P(\\tilde{y}_{\\text{tree photo}} \\mid y^*_{\\text{flower photo}}) > P(\\tilde{y}_{\\text{tree photo}} \\mid y^*_{\\text{computer photo}})\n",
    "$$\n",
    "\n",
    "This inequality implies that mislabeling between classes such as trees and flowers (which share similarities) is more likely compared to mislabeling between trees and computers (which are very different).\n",
    "\n",
    "### Real-World Application\n",
    "\n",
    "An illustrative case comes from a project I led with the Brazilian National Council of Justice and the United Nations Development Programme:\n",
    "\n",
    "- **Application:** Classification of legal data related to environmental crimes.\n",
    "- **Data Volume:** 135,668 data points across 20 classification tasks.\n",
    "- **Efficiency:** A single legal expert created the labeling functions, completing the process in just a few days.\n",
    "- **Performance:** The generated labels achieved a correctness rate between 93% and 100%, exceeding the quality found in benchmark datasets referenced in [Northcutt et al, 2021](http://arxiv.org/abs/2103.14749).\n",
    "\n",
    "This example shows how weak supervision can be effectively applied even in specialized fields with minimal expert input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Takeaways\n",
    "- **Data Quality Over Model Complexity:**  \n",
    "  The performance of AI systems greatly benefits from investments in data curation, augmentation, and governance rather than continuous innovations in model design. Simple improvements in the input data can drive significant performance gains.\n",
    "\n",
    "- **Expandable and Cost-Efficient Approaches:**  \n",
    "  Embracing programmatic labeling and weak supervision techniques allows for the creation of large, effective datasets without the heavy costs and time commitments of manual labeling. This makes AI development more practical for large-scale and sensitive applications.\n",
    "\n",
    "- **Collaborative and Iterative Process:**  \n",
    "  Combining automated methods with continuous SME input leads to more nuanced, accurate models. Regular feedback and teamwork not only simplify the labeling process but also ensure that AI systems remain aligned with real-world requirements.\n",
    "\n",
    "- **Adaptability in a Dynamic Environment:**  \n",
    "  The data-centric approach supports continuous updates and improvements. This adaptability is vital in domains where information evolves rapidly, ensuring that AI systems remain relevant and accurate over time.\n",
    "\n",
    "- **Broad Applicability of Weak Supervision:**  \n",
    "  Empirical evidence and theoretical insights demonstrate that weak supervision does not only serve as a stopgap when labeled data is scarce, it can even outperform traditional methods in various contexts by effectively utilizing large volumes of weakly labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the main shift in focus in modern AI development, moving from model-centric to data-centric approaches, and why is this shift occurring?\n",
    "\n",
    "2.  Contrast the conventional model-centric approach to AI development with the emerging data-centric approach, highlighting their primary focuses and assumptions.\n",
    "\n",
    "3.  Explain why programmatic labeling is becoming a necessary principle in data-centric AI, outlining the limitations of manual data labeling that it addresses.\n",
    "\n",
    "4.  Describe the essential role of Subject Matter Experts (SMEs) in data-centric AI development and how their involvement enhances the quality and relevance of AI systems.\n",
    "\n",
    "5.  According to the principles of data-centric AI, what are the key strategies for enhancing AI development by focusing on data, beyond just collecting more data?\n",
    "\n",
    "6.  Define weak supervision regarding machine learning and explain its purpose in overcoming the limitations of fully supervised learning.\n",
    "\n",
    "7.  Categorize and briefly describe the three main types of weak supervision as defined in the notebook content.\n",
    "\n",
    "8.  Explain the concept of programmatic labeling functions in weak supervision and how they contribute to efficient data labeling.\n",
    "\n",
    "9.  What is the key trade-off between data quantity and quality in weak supervision, and how is comparable performance achieved despite potentially noisy labels?\n",
    "\n",
    "10.  Based on the notebook content, what are the key benefits of adopting a data-centric approach to AI development, and how does it impact the overall AI development lifecycle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell.`\n",
    "\n",
    "<!-- 1. The main shift is from focusing primarily on model architecture improvements to emphasizing data quality and scale. This shift is occurring because significant performance improvements are now being realized through better and more abundant training data, even with relatively simpler models.\n",
    "\n",
    "2. The model-centric approach prioritizes developing and improving model architectures, with data being secondary. The data-centric approach, conversely, emphasizes that substantial AI performance improvements come from high-quality, large datasets, making data handling of primary importance.\n",
    "\n",
    "3. Programmatic labeling is necessary due to the challenges of manual labeling, including limitations in scalability, cost inefficiency, time constraints, and difficulties in handling data privacy, specialized expertise needs, and evolving information.\n",
    "\n",
    "4. Subject Matter Experts (SMEs) are essential because their domain knowledge refines data interpretation, feature engineering, and model evaluation, ensuring that model outputs are accurate and practically relevant. They provide context beyond algorithms and data alone.\n",
    "\n",
    "5. Key strategies include focusing on data quality, data augmentation and synthesis, efficient data labeling methods like weak supervision, data governance and ethics, and continuous data improvement through regular updates and assessments.\n",
    "\n",
    "6. Weak supervision refers to training models with labels that are incomplete, inexact, or inaccurate compared to fully supervised learning. It aims to reduce the cost and effort of obtaining large, high-quality labeled datasets.\n",
    "\n",
    "7. The three types are: Incomplete Supervision, where only a subset of data is labeled; Inexact Supervision, where labels are imprecise or aggregated (e.g., multiple instance learning); and Inaccurate Supervision, where labeled data contains noise and errors.\n",
    "\n",
    "8. Programmatic labeling functions involve using rules, heuristics, or automated methods based on domain knowledge to assign labels to data points. This allows for rapid generation of labels, making the labeling process more efficient and adaptable compared to manual labeling.\n",
    "\n",
    "9. The trade-off is that weak supervision may require a larger dataset compared to fully supervised learning. However, by utilizing aggregation techniques and probabilistic labels to adjust for uncertainty, comparable performance can be achieved with significantly reduced labeling costs and time.\n",
    "\n",
    "10. Key benefits include improved AI system performance through better data, expandable and cost-efficient data creation, enhanced model robustness and reliability, adaptability to dynamic environments, and a shift towards a more practical and efficient AI development lifecycle centered around data quality and iterative improvement. -->\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
