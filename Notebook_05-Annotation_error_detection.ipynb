{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Error Detection\n",
    "## IMD3011 - Datacentric AI\n",
    "### [Dr. Elias Jacob de Menezes Neto](https://docente.ufrn.br/elias.jacob)\n",
    "\n",
    "\n",
    "> Parts of this notebook were adapted from the amazing [MIT Introduction to Data-Centric AI](https://dcai.csail.mit.edu) course taugh by the amazing folks from [Cleanlab](https://cleanlab.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "\n",
    "- **Annotation Error Detection (AED):** AED is a systematic process for identifying mislabeled or inconsistent examples in datasets. It employs two primary approaches: binary flaggers (which mark potential errors) and scorers (which assign confidence scores indicating the likelihood of error). These techniques are essential for ensuring high-quality training data.\n",
    "\n",
    "- **Types of Label Noise:** Label noise manifests in three distinct patterns:\n",
    "  - **Uniform (Symmetric) Noise:** Every incorrect label is equally likely regardless of the true class\n",
    "  - **Systematic (Asymmetric) Noise:** Specific classes are consistently confused with other particular classes\n",
    "  - **Instance-Dependent Noise:** The probability of mislabeling varies based on the specific features of each instance\n",
    "\n",
    "- **Label Noise Transition Matrix:** This matrix represents the conditional probability p(á»¹=i|y*=j) that an instance with true label j is observed as label i. It provides a mathematical framework for understanding noise patterns and forms the foundation for methods like Confident Learning that adjust for label noise during training.\n",
    "\n",
    "- **Causes of Label Noise:** Noisy labels stem from various sources including human error (fatigue, lack of expertise), measurement issues (faulty tools, inconsistent protocols), algorithmic errors (propagation from earlier models), data corruption (technical glitches), and subjective interpretations (ambiguity, cultural differences).\n",
    "\n",
    "- **Impact of Noisy Labels:** Label noise significantly degrades model performance by causing models to learn incorrect associations, overfit to noise patterns, and produce skewed performance metrics. Even small percentages of mislabeled data can have outsized effects, especially in critical applications.\n",
    "\n",
    "- **Retagging Approach:** This technique uses predictions from held-out models to identify and correct potential mislabels by comparing model outputs with existing labels. It implements a cross-validation strategy to ensure predictions are out-of-sample, reducing the risk of reinforcing existing errors.\n",
    "\n",
    "- **Model Confidence and Thresholding:** Effective AED relies on establishing class-specific confidence thresholds through proper calibration. This allows for identifying examples whose predicted probabilities deviate significantly from expected distributions, isolating potentially erroneous instances.\n",
    "\n",
    "- **Types of Uncertainty:** The notebook distinguishes between aleatoric uncertainty (irreducible randomness in the data) and epistemic uncertainty (limitations in model knowledge). Understanding these types helps in diagnosing whether errors stem from inherent data noise or model limitations.\n",
    "\n",
    "- **Data-Centric vs. Model-Centric Methods:** While model-centric approaches modify loss functions to account for noise (through importance reweighting or loss correction), data-centric methods focus directly on improving data quality through identification and correction of label errors. The notebook emphasizes data-centric approaches as particularly valuable for weak supervision scenarios.\n",
    "\n",
    "- **Prevalence in Real Datasets:** Research by Northcutt et al. (2021) revealed that even highly regarded datasets contain mislabeled examples, with error rates ranging from 0.15% (MNIST) to over 10% (QuickDraw). This highlights the pervasiveness of label noise across machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "By the end of this class, you will be able to:\n",
    "\n",
    "1.  **Explain** the fundamental concepts of label noise, including its different types (uniform, systematic, instance-dependent), common sources, and significant impacts on machine learning model performance and evaluation.\n",
    "\n",
    "2.  **Analyze** the mathematical formalisms for modeling noisy labels, including joint probability distributions, the label noise transition matrix, and the distinction between observed (noisy) labels and true (latent) labels.\n",
    "\n",
    "3.  **Distinguish** between aleatoric and epistemic uncertainty and evaluate their roles in understanding prediction errors and identifying potential label noise.\n",
    "\n",
    "4.  **Compare and contrast** model-centric and data-centric strategies for addressing label noise, focusing on the principles behind data-centric approaches like Annotation Error Detection (AED).\n",
    "\n",
    "5.  **Apply** the retagging methodology using out-of-sample predictions (e.g., via cross-validation) to identify and potentially correct mislabeled examples within a dataset.\n",
    "\n",
    "6.  **Implement** a practical pipeline for AED using standard machine learning tools (like Scikit-learn and TfidfVectorizer) to preprocess text data, train models, generate predictions, and identify label discrepancies.\n",
    "\n",
    "7.  **Evaluate** the effectiveness of an AED technique (like retagging) by comparing model performance before and after applying the data cleaning process, using appropriate classification metrics.\n",
    "\n",
    "8.  **Critically assess** the challenges inherent in AED, such as the lack of ground truth, class imbalance, and the difficulty of distinguishing true errors from ambiguous cases.\n",
    "\n",
    "9.  **Integrate** AED concepts within a broader weak supervision framework, understanding how AED can refine weakly labeled datasets to improve final model quality.\n",
    "\n",
    "10. **Analyze** research findings on the prevalence of label noise in standard benchmark datasets and articulate the implications for model development and reliable evaluation in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"images/wsl04_01.png\" width=\"80%\" height=\"80%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation for Noisy Labels\n",
    "\n",
    "Understanding the notations used in discussions about noisy labels and their true counterparts is important for grasping the basic concepts. This section introduces key symbols and definitions that help describe the relationship between observed labels and true labels regarding label noise.\n",
    "\n",
    "### Key Notations and Definitions\n",
    "\n",
    "- **$\\tilde{y}$**: This symbol represents the **observed label**, which might contain errors due to noise in the data collection process.\n",
    "\n",
    "- **$y^*$**: This denotes the **true label** (also called the latent or correct label). The true label is what the observed label $\\tilde{y}$ should be if no mistakes occurred during data collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Sets and Counts\n",
    "\n",
    "The following notations help describe and quantify the relationship between noisy observed labels and true labels:\n",
    "\n",
    "- **$X_{\\tilde{y}=i,\\, y^*=j}$**: This set contains all examples for which the observed label is $i$ and the true label is $j$. Analyzing these sets allows you to identify patterns of labeling errors.\n",
    "\n",
    "- **$C_{\\tilde{y}=i,\\, y^*=j}$**: This count represents the number of instances in the set $X_{\\tilde{y}=i,\\, y^*=j}$. These counts are essential for calculating the distribution of label noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Probabilities and Distributions\n",
    "\n",
    "A statistical description of noisy labels requires modeling the probabilities of various label outcomes. Two key concepts are:\n",
    "\n",
    "- **Joint Probability Distribution**  \n",
    "  The joint probability $p(\\tilde{y}=i,\\, y^*=j)$ is defined as the probability that an instance is observed with label $i$ and its true label is $j$. This is computed by normalizing the counts:\n",
    "  \n",
    "  $$\n",
    "  p(\\tilde{y}=i,\\, y^*=j) = \\frac{C_{\\tilde{y}=i,\\, y^*=j}}{N}\n",
    "  $$\n",
    "  \n",
    "  where $N$ is the total number of instances in the dataset.\n",
    "\n",
    "- **Transition Probability**  \n",
    "  The conditional probability $p(\\tilde{y}=i \\mid y^*=j)$ indicates the probability that an instance with true label $j$ is observed as label $i$. It quantifies the chance of mislabeling:\n",
    "  \n",
    "  $$\n",
    "  p(\\tilde{y}=i \\mid y^*=j) = \\frac{C_{\\tilde{y}=i,\\, y^*=j}}{C_{y^*=j}}\n",
    "  $$\n",
    "  \n",
    "  where\n",
    "  \n",
    "  $$\n",
    "  C_{y^*=j} = \\sum_{k} C_{\\tilde{y}=k,\\, y^*=j}\n",
    "  $$\n",
    "  \n",
    "  represents the total number of instances with true label $j$.\n",
    "\n",
    "> **Important:**  \n",
    "> Distinguishing between $\\tilde{y}$ and $y^*$ allows us to adjust our models to account for label noise. Understanding and estimating the transition probability $p(\\tilde{y}=i \\mid y^*=j)$ is particularly useful for designing systems that can correct for or mitigate the impact of labeling errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Practical Example: Animal Image Classification\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/ai_cat_dog_meme.png\" width=\"40%\" height=\"40%\" />\n",
    "</p>\n",
    "\n",
    "Consider a dataset of images where each image is labeled with an animal category (e.g., cats, dogs, horses). Due to mislabeling during data collection, some images of cats might be labeled as dogs. Hereâs how the notations apply:\n",
    "\n",
    "- **Noisy Label ($\\tilde{y}$)**: The label given during data collection (e.g., \"dog\" for a misclassified cat image).\n",
    "- **True Label ($y^*$)**: The actual category to which the image belongs (e.g., \"cat\").\n",
    "\n",
    "For an image of a cat that has been mislabeled:\n",
    "\n",
    "- **$X_{\\tilde{y}=\\text{dog},\\, y^*=\\text{cat}}$**: This set contains images that were observed as \"dog\" but are actually \"cat.\"\n",
    "- **$C_{\\tilde{y}=\\text{dog},\\, y^*=\\text{cat}}$**: This count is incremented for each instance where an image is misclassified from \"cat\" to \"dog.\"\n",
    "\n",
    "Using the counts across the entire dataset, you can compute the joint distribution\n",
    "\n",
    "$$\n",
    "p(\\tilde{y}=i,\\, y^*=j)\n",
    "$$\n",
    "\n",
    "and the transition probabilities\n",
    "\n",
    "$$\n",
    "p(\\tilde{y}=i \\mid y^*=j)\n",
    "$$\n",
    "\n",
    "This approach helps in quantifying the impact of noise, ultimately guiding improvements in model performance through techniques such as noise-aware training methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of Noisy Labels\n",
    "\n",
    "Noisy labels occur due to various issues that can arise during data collection, processing, or annotation. Understanding these sources is key to mitigating their impact on model performance. This section explores common causes of label noise in datasets and their consequences for machine learning applications.\n",
    "\n",
    "\n",
    "### 1. Human Error\n",
    "\n",
    "- **Accidental Mislabeling:**  \n",
    "  During the annotation process, an annotator might select the wrong label by mistake (e.g., clicking an incorrect button in the interface).\n",
    "\n",
    "- **Fatigue or Inattention:**  \n",
    "  Extended periods of labeling can lead to decreased concentration, resulting in oversight or mistakes.\n",
    "\n",
    "- **Lack of Domain Expertise:**  \n",
    "  Annotators who are not familiar with the specific subject matter may misclassify instances due to insufficient knowledge of the nuances involved.\n",
    "\n",
    "*Example:* In a clinical diagnosis dataset, non-expert annotators might confuse similar medical conditions, leading to noisy labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Measurement Issues\n",
    "\n",
    "- **Faulty Data Collection Tools:**  \n",
    "  Inaccurate or poorly calibrated instruments can compromise the quality of the gathered data, leading to erroneous labels.\n",
    "\n",
    "- **Inconsistent Measurement Approaches:**  \n",
    "  Variability in data collection protocols or methodologies can introduce discrepancies. For example, differing guidelines across locations or time periods can result in inconsistent labeling.\n",
    "\n",
    "*Example:* Temperature sensors with calibration issues may record incorrect values, affecting the labels in a weather dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Algorithmic Errors\n",
    "\n",
    "- **Error Propagation from Earlier Models:**  \n",
    "  When labels are generated or refined using machine learning models (e.g., within a weak supervision pipeline), mistakes made by those models can be carried forward. This phenomenon is often seen when predictions from a preliminary model are used to label large datasets.\n",
    "\n",
    "- **Biases in Automated Labeling:**  \n",
    "  Algorithms may have innate biases or flaws that lead to systematic mislabeling, particularly when the training data of the automated process is itself noisy or unrepresentative.\n",
    "\n",
    "*Example:* An automated system trained on biased historical data might over-represent certain categories while under-representing others, thus skewing the label distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Corruption\n",
    "\n",
    "- **Technical Glitches:**  \n",
    "  Errors during data storage, transfer, or processing (e.g., software bugs or hardware failures) can corrupt labels.\n",
    "\n",
    "- **Malicious Tampering:**  \n",
    "  In some scenarios, datasets may be intentionally manipulated, either for competitive advantage or due to security breaches, resulting in label inaccuracies.\n",
    "\n",
    "*Example:* A distributed database might experience data corruption during synchronization, leading to mismatches between the true state and the recorded labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Subjective Interpretations\n",
    "\n",
    "- **Ambiguity in Labeling:**  \n",
    "  Certain cases might not have a clear-cut category, and different annotators could assign different labels based on their interpretation. Such ambiguity can lead to inconsistent or noisy labels.\n",
    "\n",
    "- **Cultural or Contextual Variations:**  \n",
    "  Labelers from different cultural or contextual backgrounds might interpret data differently, especially in areas like sentiment analysis or image categorization.\n",
    "\n",
    "*Example:* When annotating social media posts for sentiment, one annotator might label a sarcastic remark as positive while another could interpret it as negative, leading to noise in the training labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Flipping and Noisy Labels\n",
    "\n",
    "Label flipping happens when an instance is assigned an incorrect label, effectively swapping its true class with another. This concept is key to understanding how label errors, or *noisy labels*, can influence machine learning models.\n",
    "\n",
    "### Examples of Label Flipping\n",
    "\n",
    "- **Visual Misclassification:** An image depicting a bus is incorrectly labeled as a car.\n",
    "- **Sentiment Analysis Error:** A review with a positive tone such as \"Muito bom, gostei bastante!\" is mistakenly tagged as negative.\n",
    "- **NER Mislabeling:** A named entity like \"Elias Jacob\" is left untagged or assigned an incorrect category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Quantifying Label Noise\n",
    "\n",
    "One way to quantify label noise is by using a matrix that compares the true labels, $ y^* $, with the observed, or noisy, labels, $ \\tilde{y} $.\n",
    "\n",
    "> **Confusion Matrix Interpretation:**\n",
    ">\n",
    "> - **Diagonal elements:** The count of instances correctly labeled.\n",
    "> - **Off-diagonal elements:** The count of label flips (mislabeled instances).\n",
    "\n",
    "Consider the following matrix:\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| $ C_{\\tilde{y}, y^*} $ | $ y^* = \\text{bus} $ | $ y^* = \\text{car} $ | $ y^* = \\text{bike} $ |\n",
    "|--------------------------|------------------------|------------------------|------------------------|\n",
    "| $ \\tilde{y} = \\text{bus} $   | 90                   | 35                   | 25                   |\n",
    "| $ \\tilde{y} = \\text{car} $   | 50                   | 70                   | 5                    |\n",
    "| $ \\tilde{y} = \\text{bike} $  | 30                   | 15                   | 75                   |\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "For instance, notice that 50 instances with the true label \"bus\" have been misclassified as \"car\". This matrix helps in visualizing the extent and pattern of label noise in dataset labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Impact of Noisy Labels\n",
    "\n",
    "Noisy labels can negatively affect the training and performance of machine learning models in several ways:\n",
    "\n",
    "- **Model Performance:** Inaccurate labels can reduce model accuracy and hurt its ability to generalize to unseen data.\n",
    "- **Training Challenges:** The model may end up fitting the noise, making it difficult to identify true patterns in the data.\n",
    "- **Evaluation Concerns:** Test sets with noisy labels might lead to misleading assessments of model performance.\n",
    "- **Bias Introduction:** Systematic labeling errors can introduce or worsen bias within the model.\n",
    "- **Wasted Resources:** Training on datasets with label noise may result in inefficient use of computational time and resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Uncertainty in Predictions\n",
    "\n",
    "In machine learning, **uncertainty** indicates the extent to which a model lacks confidence in its predictions. It highlights the possibility of model errors and helps assess the reliability of outputs. Two primary types of uncertainty influence model predictions:\n",
    "\n",
    "### 1. Aleatoric Uncertainty\n",
    "\n",
    "- **Definition:** This uncertainty is due to innate randomness or noise present in the data. It is considered irreducible because it originates from the data-generation process.\n",
    "- **Example:** In predicting house prices, factors such as sudden market variations or minor differences in property features contribute to aleatoric uncertainty. Even with extensive data, predicting the exact price is challenging because of these unavoidable variations.\n",
    "\n",
    "### 2. Epistemic Uncertainty\n",
    "\n",
    "- **Definition:** This uncertainty comes from limitations in the model's understanding of the relationship between inputs and outputs. It reflects the gap in the modelâs knowledge and can be reduced by improving the model, such as by including more extensive or diverse training examples.\n",
    "- **Example:** Consider a model trained primarily on images of adult cats and dogs. When faced with images of kittens or puppies, the model may show higher uncertainty because these examples were not well-represented in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Disentangling Label Noise and Uncertainty\n",
    "\n",
    "In practical scenarios, distinguishing whether errors stem from label noise (related to aleatoric uncertainty) or from the model's limitations (linked to epistemic uncertainty) is not straightforward. To address this, researchers often introduce a **label noise process assumption**.\n",
    "\n",
    "### Class-Conditional Label Noise Model\n",
    "\n",
    "A common assumption is that the label noise depends only on the true class label and not on other features of the input. This assumption can be described by:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y} \\mid y^*; x) = p(\\tilde{y} \\mid y^*)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\tilde{y} $ is the observed (possibly incorrect) label,\n",
    "- $ y^* $ is the true label,\n",
    "- $ x $ represents the input features.\n",
    "\n",
    "This model implies that for all data points belonging to the same true class, the probability of a label flip follows the same pattern, regardless of individual differences in the input features. The transition probabilities $ p(\\tilde{y} \\mid y^*) $ can be organized into a matrix similar to the confusion matrix shown earlier. This helps in isolating the effects of noisy labels from the shortcomings related to the model's current design or training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Label Noise in Machine Learning\n",
    "\n",
    "Label noise can considerably affect model performance. It is important to recognize the different types of label noise to build models that can handle or correct these errors. The three primary types are **uniform (symmetric) noise**, **systematic (asymmetric) class-conditional noise**, and **instance-dependent noise**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Uniform (Symmetric) Label Noise\n",
    "\n",
    "**Definition:**  \n",
    "Uniform label noise assumes that for any instance with the true label $ y^* $, the probability of it being mislabeled as any other label is the same across all incorrect options. Formally, for any incorrect label $ i $ and true label $ j $:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y} = i \\mid y^* = j) = \\epsilon, \\quad \\forall \\, i \\neq j\n",
    "$$\n",
    "\n",
    "where $ \\epsilon $ is the noise rate.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- **Simplicity:** It is the most straightforward noise model.\n",
    "- **Uniform Distribution:** Errors are assumed to be uniformly distributed among all classes.\n",
    "- **Baseline Model:** Frequently used as a baseline in studies that focus on building noise-reliable techniques.\n",
    "\n",
    "**Consequences:**\n",
    "\n",
    "- **Modeling:** Easy to model and can serve as an initial test case.\n",
    "- **Limitations:** This model might not capture the error patterns observed in real-world datasets, where mislabeling often shows bias toward certain classes.\n",
    "\n",
    "> **Note:** Although common in research settings, this model may oversimplify the complexity of noise in practical applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Systematic (Asymmetric) Class-Conditional Label Noise\n",
    "\n",
    "**Definition:**  \n",
    "Systematic noise allows the mislabeling probabilities to vary between different class pairs. The probability of a mislabel depends on the true label $ y^* $ and can vary for each incorrect label $ i $:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y} = i \\mid y^* = j) = \\epsilon_{ij}\n",
    "$$\n",
    "\n",
    "with different values of $ \\epsilon_{ij} $ for each pair $ (i, j) $.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- **Realism:** More closely reflects actual noise in many datasets.\n",
    "- **Error Patterns:** Often represents common misclassification trends, such as confusion between visually similar classes.\n",
    "\n",
    "**Consequences:**\n",
    "\n",
    "- **Complexity:** Modeling these probabilities requires a more complex approach.\n",
    "- **Effectiveness:** Better captures the nuances in data labeling processes.\n",
    "  \n",
    "For example, in an animal image classification task, a cat might be more frequently misclassified as a dog rather than as a bird, reflecting a higher $p(\\tilde{y} = \\text{dog} \\mid y^* = \\text{cat})$ than $p(\\tilde{y} = \\text{bird} \\mid y^* = \\text{cat})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instance-Dependent Label Noise\n",
    "\n",
    "**Definition:**  \n",
    "Instance-dependent noise takes into account both the true class $ y^* $ and the specific features $ x $ of the instance. The probability of an incorrect label is represented as:\n",
    "\n",
    "$$\n",
    "p(\\tilde{y} = i \\mid y^* = j, x)\n",
    "$$\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- **Detail-Oriented:** It considers the attributes of individual samples, meaning that noise levels can vary across instances from the same class.\n",
    "- **Complexity:** This model is the most detailed and can better mimic real-world scenarios where certain features make an instance more likely to be mislabeled.\n",
    "\n",
    "**Consequences:**\n",
    "\n",
    "- **Modeling Difficulty:** Requires strong assumptions about the distribution of both the features and the labels.\n",
    "- **Practicality:** While realistic, handling instance-dependent noise often proves challenging due to the added complexity.\n",
    "\n",
    "For example, in an image classification task, a blurry or occluded image (specific instance) of a cat may be much more likely to be mislabeled than a clear, well-lit image of the same cat.\n",
    "\n",
    "> **Important:** Although instance-dependent noise may fit actual label noise patterns more accurately, its complexity usually makes it less practical for many applications. Researchers often need to balance realism and tractability when choosing a noise model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Example: Image Classification Task\n",
    "\n",
    "Consider an animal classification problem with the following scenarios:\n",
    "\n",
    "1. **Uniform Noise:** A cat has an equal chance of being mislabeled as a dog, bird, or any other animal.\n",
    "2. **Asymmetric Noise:** A cat is more likely to be confused with a dog rather than a bird, reflecting visual similarities between cats and dogs.\n",
    "3. **Instance-Dependent Noise:** A poorly contrasted or blurry image of a cat is more likely to be mislabeled compared to a high-quality image of a cat.\n",
    "\n",
    "In real-world datasets like ImageNet, we often observe asymmetric label noise. For example, many images of **wild boars** are mislabeled as **pigs**, and vice versa. This is a clear example of systematic noise, where certain classes are more likely to be confused due to their visual similarities.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/label_errors_pig.png\" width=\"100%\" height=\"100%\" alt=\"Wild Boar and Pig Label Errors\"/>\n",
    "</p>\n",
    "\n",
    "If label noise was uniform, we would expect the mislabeling to be evenly distributed across all classes. However, the systematic nature of the noise indicates that certain classes are more likely to be confused with specific others, leading to asymmetric noise patterns.\n",
    "\n",
    "> **Note**: Despite the prevalence of asymmetric label noise in real-world datasets, many noise-resistant learning studies still rely on the uniform noise assumption due to its simplicity and ease of modeling. This usually results in myths like \"neural networks are reliable to label noise\" because they are often tested on datasets with uniform noise, which doesn't accurately reflect real-world scenarios. This discrepancy between research assumptions and real-world noise patterns highlights the importance of developing noise-handling techniques that can address more complex noise structures.\n",
    "\n",
    "For more examples, [check this website](https://labelerrors.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sources of Noise in Machine Learning\n",
    "\n",
    "### Noise in Data vs. Noise in Labels\n",
    "\n",
    "Machine learning models deal with different types of noise. It is important to separate **noise in input data** from **noise in labels**.\n",
    "\n",
    "- **Noise in Input Data**  \n",
    "  Common types include:  \n",
    "  - **Visual Noise**: Distortion, blur, or occlusion in images.  \n",
    "    - *Example*: A low-resolution image of a sidewalk where details are smeared.  \n",
    "  - **Adversarial Examples**: Inputs that have been deliberately altered to trick models.  \n",
    "    - *Example*: A slightly modified image of a car that a model incorrectly identifies as a bicycle.  \n",
    "  - **Textual Noise**: Typos or grammatical errors in text, which can confuse language models.  \n",
    "    - *Example*: A sentence with misspelled words or misplaced commas.  \n",
    "  - **Audio Noise**: Unwanted background sounds or distortions in audio recordings.  \n",
    "    - *Example*: A conversation recorded with heavy traffic background noise.\n",
    "\n",
    "- **Label Noise**  \n",
    "  This type of noise occurs during the labeling process when annotators assign incorrect or inconsistent labels.  \n",
    "  - *Example*: An image of a toy car might receive the label \"Sports Car\" from one annotator and \"Toy Car\" from others.  \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "  > \n",
    "  > **Key Assumptions in Confident Learning (CL):**\n",
    "  > 1. The noise exists in the labels rather than the data itself.\n",
    "  > 2. Each example is assumed to have a single annotation.\n",
    "  \n",
    "These assumptions simplify the analysis by isolating the label noise and allowing the methods to focus on reducing errors in class assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for Handling Noisy Labels\n",
    "\n",
    "There are two main strategies to address label noise:\n",
    "\n",
    "### 1. Model-Centric Methods: \"Change the Loss\"\n",
    "\n",
    "These approaches modify the training process to account for noisy labels within the loss function.\n",
    "\n",
    "- **Using Loss from Another Network**  \n",
    "  In methods like *Co-Teaching*, multiple models are trained at the same time. Each model assists in identifying and reducing the impact of mislabeled examples on the other.  \n",
    "  *Example*: Two neural networks learn from the same dataset, but each selects a subset of examples it deems to have reliable labels for the partner network.\n",
    "\n",
    "- **Direct Loss Modification**  \n",
    "  Here, the standard loss function is modified to be less sensitive to label noise.  \n",
    "  - **Symmetric Cross Entropy (SCE) Loss** is one variant that combines cross entropy with a reverse cross entropy term:\n",
    "    $$\n",
    "    \\text{SCE Loss} = \\alpha \\cdot \\text{Cross Entropy} + \\beta \\cdot \\text{Reverse Cross Entropy}\n",
    "    $$\n",
    "    where $\\alpha$ and $\\beta$ control the balance between the two components.\n",
    "\n",
    "- **Importance Reweighting**  \n",
    "  This method assigns different weights to training examples based on the likelihood that each label is accurate.  \n",
    "  - One may modify the loss as:\n",
    "    $$\n",
    "    \\text{Loss} = \\sum_{i=1}^{N} w_i \\cdot L\\big(y_i, f(x_i)\\big)\n",
    "    $$\n",
    "    where $w_i$ represents the weight for example $i$, $L$ is the loss function (such as cross entropy), $y_i$ is the label, and $f(x_i)$ is the networkâs prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Data-Centric Methods: \"Change the Data\"\n",
    "\n",
    "These techniques address label noise by enhancing the quality of the training set.\n",
    "\n",
    "- **Identifying Label Errors**  \n",
    "\n",
    "  Techniques focus on detecting and flagging potential mislabeled examples. Methods include:\n",
    "  - **Statistical Analysis**: Checking for data points whose predicted labels deviate significantly from the provided labels.\n",
    "\n",
    "  - **Model Predictions**: Using trained models to identify samples with high disagreement between the prediction and the given label.\n",
    "\n",
    "  - **Manual Review**: Involving experts to inspect and confirm labels for ambiguous cases.\n",
    "\n",
    "- **Learning with Cleaned Data**  \n",
    "\n",
    "  Once candidate label errors are identified, the dataset can be adjusted by:\n",
    "  - Removing mislabeled examples.\n",
    "\n",
    "  - Correcting labels when possible.\n",
    "\n",
    "  - Assigning lower weights to examples suspected of being noisy during training.\n",
    "\n",
    "> **Our Focus:** Since you are in a Datacentric AI course, we will focus on data-centric methods, which emphasize improving the quality of training data to enhance model performance. These techniques are particularly relevant in weak supervision scenarios, where the training data may contain significant label noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Having established the importance of data-centric methods for handling noisy labels (particularly through the identification and correction of label errors) we now turn our attention to the specialized techniques that make this possible. Detecting mislabeled instances is a critical step in improving data quality, which directly impacts the performance and reliability of machine learning models.\n",
    "> \n",
    "> This brings us to `Annotation Error Detection (AED)`, a collection of methodologies designed to systematically identify potential errors in data annotations. AED plays a critical role in the data curation process by ensuring that models are trained on accurate and trustworthy data. Focusing on the annotations themselves, AED methods complement the data-centric approach by providing tools to clean and refine datasets effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Annotation Error Detection (AED)\n",
    "\n",
    "Even carefully curated datasets can contain errors or inconsistencies in their annotations. AED focuses on automatically identifying these potential errors, helping human annotators and dataset creators to improve data quality. \n",
    "\n",
    "## Types of Annotation Errors\n",
    "\n",
    "AED identifies several types of annotation mistakes:\n",
    "\n",
    "1. **Incorrect Labels:**  \n",
    "   These occur when the given label is wrong. For instance, a positive review might be labeled as negative in sentiment analysis.\n",
    "\n",
    "2. **Inconsistencies:**  \n",
    "   Similar items may be labeled differently across a dataset. In tasks like named entity recognition, the same person's name might be tagged as a person in some cases but not in others due to variable interpretations or changes in guidelines.\n",
    "\n",
    "3. **Ambiguities:**  \n",
    "   These arise when data can reasonably be interpreted in more than one way, but the annotation scheme permits only one label. An example is a sentence that could be seen as both sarcastic and sincere in sentiment analysis.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/label_errors1.png\" width=\"100%\" height=\"100%\" />\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<a href=\"https://labelerrors.com\">Source</a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Categories and Techniques of AED Methods\n",
    "\n",
    "AED methods generally fall into two classes:\n",
    "\n",
    "1. **Flaggers:**  \n",
    "   These methods output a binary decision indicating whether an instance is likely correct or erroneous.\n",
    "   \n",
    "2. **Scorers:**  \n",
    "   These methods assign a probability score to each instance, indicating the likelihood that the annotation is in error. For example, instances with higher scores require more attention.\n",
    "\n",
    "Various techniques help in detecting annotation errors:\n",
    "\n",
    "- **Model-Based Methods:**  \n",
    "  A machine learning model (such as a classifier) is trained on a subset of data to assess the probability that an instance is misannotated.\n",
    "\n",
    "- **Variation-Based Methods:**  \n",
    "  These techniques compare similar items. If similar items receive different labels, it may signal an error in one of the annotations.\n",
    "\n",
    "- **Ensemble Methods:**  \n",
    "  Combining multiple AED methods can improve overall accuracy by taking advantage of the strengths of each approach.\n",
    "\n",
    "- **Vector Space Proximity Methods:**  \n",
    "  Dense embeddings are used to map instances into a vector space. If an instance with a certain label lies far from other similar instances, this anomaly can indicate an annotation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applications of AED\n",
    "\n",
    "AED has practical use cases across different machine learning tasks, such as:\n",
    "\n",
    "- **Document Classification:**  \n",
    "  Detecting misclassified documents.\n",
    "\n",
    "- **Named Entity Recognition:**  \n",
    "  Identifying inconsistent or incorrect entity tags.\n",
    "\n",
    "- **Image Classification:**  \n",
    "  Flagging images that might have wrong labels.\n",
    "\n",
    "- **Pixel-Wise Segmentation:**  \n",
    "  Highlighting image regions where annotations may be incorrect.\n",
    "\n",
    "- **Regression Tasks:**  \n",
    "  Detecting outliers or points with potentially misannotated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Formal Definition and Mathematical Formulation\n",
    "\n",
    "Given an annotated dataset  \n",
    "$$\n",
    "D = \\{(x_i, \\tilde{y_i})\\}_{i=1}^{N},\n",
    "$$  \n",
    "where $x_i$ is an input (such as a sentence or token) and $\\tilde{y_i}$ is the observed label, the goal of AED is to identify a subset  \n",
    "$$\n",
    "E \\subseteq D\n",
    "$$  \n",
    "such that for each $(x_i, \\tilde{y_i}) \\in E$, the observed label $\\tilde{y_i}$ likely differs from the unknown true label $y_i^*$.\n",
    "\n",
    "Depending on the AED method, the operations are defined as:\n",
    "\n",
    "1. **For Flaggers:**  \n",
    "   $$\n",
    "   f(x_i, \\tilde{y_i}) = \n",
    "   \\begin{cases}\n",
    "   1 & \\text{if } (x_i, \\tilde{y_i}) \\text{ is likely erroneous}, \\\\\n",
    "   0 & \\text{otherwise.}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "2. **For Scorers:**  \n",
    "   $$\n",
    "   f(x_i, \\tilde{y_i}) = s_i \\quad \\text{with} \\quad s_i \\in [0, 1],\n",
    "   $$\n",
    "   where a higher value of $s_i$ indicates a greater chance that $\\tilde{y_i}$ is incorrect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An objective of AED methods can be described as:\n",
    "$$\n",
    "\\max_f \\, \\text{Metric}\\left(\\{(x_i, \\tilde{y_i}) \\mid f(x_i, \\tilde{y_i}) = 1\\}, E_{\\text{true}}\\right)\n",
    "$$\n",
    "where $E_{\\text{true}}$ is the set of truly erroneous instances, generally unknown in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Components of the AED Task\n",
    "\n",
    "AED tasks involve several components:\n",
    "\n",
    "1. **Input:**  \n",
    "   An annotated dataset is provided with only the observed labels $\\tilde{y_i}$. There is no direct access to the true labels $y_i^*$, making error detection a challenging weakly supervised task.\n",
    "\n",
    "2. **Output:**  \n",
    "   Depending on the method:\n",
    "   \n",
    "   - **Flaggers:**  \n",
    "     Provide a set of flagged instances, indicating whether they are likely mislabeled. For example, a flagger might output a list of instances with binary labels (1 for likely erroneous, 0 for likely correct).\n",
    "   \n",
    "   - **Scorers:**  \n",
    "     Provide a ranked list of instances with associated scores $s_i$ for each instance, indicating the likelihood of being mislabeled. For example, a score of $s_i = 0.9$ might indicate a high probability that the label is incorrect, while $s_i = 0.1$ suggests a low probability of error. The scores can be interpreted as confidence levels in the correctness of the labels.\n",
    "\n",
    "3. **Granularity:**  \n",
    "   AED can operate at different levels:\n",
    "   \n",
    "   - **Document or Sentence Level:** Suitable for text classification.\n",
    "   - **Token Level:** Common in tagging tasks such as part-of-speech tagging.\n",
    "   - **Span Level:** Relevant for tasks like named entity recognition.\n",
    "\n",
    "4. **Evaluation Metrics:**  \n",
    "   - **Flaggers:**  \n",
    "     Use metrics such as precision, recall, and F1 score.\n",
    "   \n",
    "   - **Scorers:**  \n",
    "     Focus on ranking metrics like average precision, Precision@k, and Recall@k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in AED\n",
    "\n",
    "AED faces several challenges:\n",
    "\n",
    "- **Lack of Ground Truth:**  \n",
    "  The true labels $y_i^*$ are unknown, making it hard to measure the exact performance of AED methods.\n",
    "\n",
    "- **Class Imbalance:**  \n",
    "  Correct labels usually outnumber erroneous ones, which may lead to biased detection methods that miss the minority class.\n",
    "\n",
    "- **Task and Domain Specificity:**  \n",
    "  Techniques that work for one task (like sentiment analysis) may not transfer directly to another (such as named entity recognition).\n",
    "\n",
    "- **Distinguishing Errors from Valid Cases:**  \n",
    "  AED methods must separate true annotation errors from edge cases or ambiguous instances where multiple interpretations may be valid. Incorrectly flagging these instances can lead to wasted manual review effort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Distinction from Related Data Quality Tasks\n",
    "\n",
    "AED is related to but distinct from tasks such as noise-robust learning and general data cleaning:\n",
    "\n",
    "- **Noise-Robust Learning:**  \n",
    "  Focuses on developing models that tolerate noisy data during training.\n",
    "\n",
    "- **Data Cleaning:**  \n",
    "  Involves directly correcting errors in the dataset.\n",
    "\n",
    "AED specifically targets discrepancies between the observed labels $\\tilde{y_i}$ and the latent true labels $y_i^*$. As such, it is a preparatory step in data curation processes, ensuring that model training and evaluation are based on more reliable data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Refining Weak Supervision with AED\n",
    "\n",
    "In weak supervision, the labels provided by a label model serve as probabilistic indicators rather than absolute truths. AED can be applied as a final step in the weak supervision pipeline to enhance label quality.\n",
    "\n",
    "### How AED Enhances Weak Supervision\n",
    "\n",
    "- **Using Weak Labels as Input:**  \n",
    "  The label model gives a weak label $\\tilde{y_i}$ along with a measure of confidence. AED accepts these probabilistic labels and evaluates them for consistency.\n",
    "\n",
    "- **Error Detection:**  \n",
    "  AED methods analyze the weak labels to spot instances where the probability suggests a deviation from the true latent label $y_i^*$.\n",
    "\n",
    "- **Dataset Refinement Strategies:**  \n",
    "  The insights provided by AED can guide further actions, such as:\n",
    "  \n",
    "  - **Manual Review and Correction:**  \n",
    "    Flagged instances can be examined by experts, who may correct the labels or offer additional clarification.\n",
    "  \n",
    "  - **Selective Data Exclusion:**  \n",
    "    Instances with a high error likelihood may be removed from the training dataset to avoid influencing the model with noisy data.\n",
    "\n",
    "### Benefits of Incorporating AED in Weak Supervision\n",
    "\n",
    "- **Improved Data Quality:**  \n",
    "  By detecting and correcting potential annotation errors, AED leads to a dataset that better represents the true basic labels.\n",
    "\n",
    "- **Higher Model Accuracy:**  \n",
    "  Models trained on an AED-refined dataset are less likely to learn from incorrect annotations, resulting in improved performance.\n",
    "\n",
    "- **Feedback on Weak Supervision:**  \n",
    "  AED can also reveal systematic issues with the weak supervision process, such as inconsistent or biased label assignments, which can guide further improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typology of Model-Probing Mislabeled Example Detection Methods\n",
    "\n",
    "The paper [proposed here](https://arxiv.org/abs/2410.15772) presents a unifying framework that connects various methods for detecting mislabeled examples in datasets by considering them as ways to probe trained machine learning models. The framework is built around four essential components, offering a clear structure that can both explain existing methods and guide the design of new ones.\n",
    "\n",
    "Before exploring AED in detail, it is important to understand what is meant by a mislabeled example. The paper discusses the definition in two settings:\n",
    "\n",
    "#### 1. Deterministic Case\n",
    "\n",
    "- **Assumption:**  \n",
    "  There exists a true, fixed function  \n",
    "  $$\n",
    "  f: X \\rightarrow Y\n",
    "  $$  \n",
    "  that assigns a correct label $ y $ to every input $ x $.\n",
    "\n",
    "- **Ideal Situation:**  \n",
    "  A perfect dataset would be  \n",
    "  $$\n",
    "  D_{\\text{noiseless}} = \\{(x_i, y^*_i = f(x_i))\\}_{i=1}^n.\n",
    "  $$\n",
    "  \n",
    "- **Realistic Scenario:**  \n",
    "  In practice, the training data is given by  \n",
    "  $$\n",
    "  D_{\\text{train}} = \\{(x_i, \\tilde{y}_i)\\}_{i=1}^n,\n",
    "  $$  \n",
    "  where some examples have labels that do not match $ f(x_i) $; these are the mislabeled examples.\n",
    "\n",
    "- **Mislabeled Definition:**  \n",
    "  An example $(x_i, \\tilde{y}_i)$ is mislabeled if  \n",
    "  $$\n",
    "  \\tilde{y}_i \\neq f(x_i).\n",
    "  $$\n",
    "\n",
    "#### 2. Stochastic Case\n",
    "\n",
    "- **Assumption:**  \n",
    "  The true concept is represented by a conditional probability distribution  \n",
    "  $$\n",
    "  P(Y \\mid X).\n",
    "  $$  \n",
    "  This means that for a given input $ x $, multiple labels might have non-zero probabilities.\n",
    "\n",
    "- **Ideal Situation:**  \n",
    "  The ideal training dataset is sampled from the joint distribution  \n",
    "  $$\n",
    "  D_{\\text{ideal}} = \\{(x_i, y^*_i)\\}_{i=1}^n,\n",
    "  $$  \n",
    "  where the labels reflect the probabilities given by $ P(Y \\mid X) $.\n",
    "\n",
    "- **Realistic Scenario:**  \n",
    "  The available training data is drawn from a possibly altered distribution  \n",
    "  $$\n",
    "  P(X, \\tilde{Y}),\n",
    "  $$  \n",
    "  meaning that some labels could be reported with probabilities different from those in the true distribution.\n",
    "\n",
    "- **Mislabeled Definition with Threshold:**  \n",
    "  Defining a mislabeled example requires additional care. Consider a threshold $ \\tau $ (with $ 0 \\leq \\tau \\leq 1 $). An example $(x, y)$ is considered mislabeled if  \n",
    "  $$\n",
    "  P(Y = \\tilde{y} \\mid X = x) < \\tau.\n",
    "  $$  \n",
    "  This approach accommodates situations where a label might be technically plausible but is unlikely compared to other possibilities. The deterministic case is simply a special situation where the conditional probability is concentrated entirely on one label.\n",
    "\n",
    "### Practical Relevance\n",
    "\n",
    "- **Trust Scores:**  \n",
    "  In practice, methods use trust scores that approximate the conditional probability $ P(Y = \\tilde{y} \\mid X = x) $. These scores help identify examples whose labels are unlikely given the input data.\n",
    "\n",
    "- **Real Case Example:**  \n",
    "  Imagine a medical diagnosis system where $ X $ represents patient data and $ Y $ represents the diagnosis. In a deterministic view, each symptom profile would point to one correct diagnosis. However, in a more realistic stochastic view, there might be uncertainty because several diagnoses could explain the symptoms with different probabilities. If a patient's data shows a 99% chance for Diagnosis A and a 1% chance for Diagnosis B, and the label assigned is Diagnosis B, a threshold-based approach would mark this record as potentially mislabeled if $ \\tau $ is set above 1%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Dataset\n",
    "\n",
    "We'll load the dataset from our WSL Pipeline Notebook and perform Annotation Error Detection to identify potential errors or inconsistencies in the labels. This process will help refine the dataset and enhance the quality of the training data for downstream tasks.\n",
    "\n",
    "We'll compare the performance of different AED methods and evaluate their effectiveness producing a dataset that is more capable of training high-performing machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_parquet(\"data/b2w/test_cleaned_with_labels.parquet\")\n",
    "df_train = pd.read_parquet(\"outputs/ws-pipeline/df_train_weakly_labeled.parquet\")\n",
    "df_dev = pd.read_parquet(\"data/b2w/dev.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label_snorkel",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label_majority_vote",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "226c2b4a-b3e9-4bb6-9321-f1fb2a9bab1f",
       "rows": [
        [
         "0",
         "b2w",
         "79d9a98a62d9adff5e2c8e2bed824e4d524695e0a1e2353a9bf995d8732dd7d3_18450635",
         "nao gostei do produto! - o acabamento e muito bom. mas a casa nao fica montada! encaixes frouxos! ja tentei colar com tudo! nao consegui! pelo preco esperava muito mais!",
         "1",
         "1"
        ],
        [
         "1",
         "b2w",
         "5177b7800f360f47ccd69afa43def2180777c1f6a3b26d6820808ac8b8a48903_32353831",
         "produto nao funciona - produto nao funcio, veio com problema na saida para instalar a camera de re e tem mais de uma semana que estou tentando trocar o produto e nao consigo, passam de uma empresa para outra e nao resolvem nada. nao recomendo nem comprar mais nesse site, pois nao oferece assistencia.",
         "0",
         "0"
        ],
        [
         "2",
         "b2w",
         "fc0cc6d9c2e4539762936bcb5b0e855df6b5e08229b00d0d40e75a47dc77e103_28697857",
         "nao recebi, portanto nao conheco o produto - paguei esse produto em 16-02 e ainda nao recebi. parece que houve problema nos correios, mas a vendedora nao me passa noticias do produto.",
         "0",
         "0"
        ],
        [
         "3",
         "b2w",
         "b3a8b907623ceece9aef15896c82a6ea3d932be9f8a856a581c8ecb89172ba05_132475745",
         "maravilhoso - parabens pela eficiencia na entrega,o produto tambem superou toda expectativa!",
         "1",
         "1"
        ],
        [
         "4",
         "b2w",
         "5e1611aae145617b04b247314421e2d65ef9d7800b2e955dde2d5d8656d0b421_24124006",
         "decepcionado - relogio com a mesma qualidade de uma casca de ovo! partes e pecas simplesmente soltaram-se com 5 minutos de uso! sinto-me traido pelas americanas.com por divulgar um produto de tao baixa qualidade, pois tenho certeza que ela nao quer ser um novo aliexpress.",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_snorkel</th>\n",
       "      <th>label_majority_vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b2w</td>\n",
       "      <td>79d9a98a62d9adff5e2c8e2bed824e4d524695e0a1e235...</td>\n",
       "      <td>nao gostei do produto! - o acabamento e muito ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b2w</td>\n",
       "      <td>5177b7800f360f47ccd69afa43def2180777c1f6a3b26d...</td>\n",
       "      <td>produto nao funciona - produto nao funcio, vei...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2w</td>\n",
       "      <td>fc0cc6d9c2e4539762936bcb5b0e855df6b5e08229b00d...</td>\n",
       "      <td>nao recebi, portanto nao conheco o produto - p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b2w</td>\n",
       "      <td>b3a8b907623ceece9aef15896c82a6ea3d932be9f8a856...</td>\n",
       "      <td>maravilhoso - parabens pela eficiencia na entr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b2w</td>\n",
       "      <td>5e1611aae145617b04b247314421e2d65ef9d7800b2e95...</td>\n",
       "      <td>decepcionado - relogio com a mesma qualidade d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                          review_id  \\\n",
       "0    b2w  79d9a98a62d9adff5e2c8e2bed824e4d524695e0a1e235...   \n",
       "1    b2w  5177b7800f360f47ccd69afa43def2180777c1f6a3b26d...   \n",
       "2    b2w  fc0cc6d9c2e4539762936bcb5b0e855df6b5e08229b00d...   \n",
       "3    b2w  b3a8b907623ceece9aef15896c82a6ea3d932be9f8a856...   \n",
       "4    b2w  5e1611aae145617b04b247314421e2d65ef9d7800b2e95...   \n",
       "\n",
       "                                                text  label_snorkel  \\\n",
       "0  nao gostei do produto! - o acabamento e muito ...              1   \n",
       "1  produto nao funciona - produto nao funcio, vei...              0   \n",
       "2  nao recebi, portanto nao conheco o produto - p...              0   \n",
       "3  maravilhoso - parabens pela eficiencia na entr...              1   \n",
       "4  decepcionado - relogio com a mesma qualidade d...              0   \n",
       "\n",
       "   label_majority_vote  \n",
       "0                    1  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    1  \n",
       "4                    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estabilish a reference point for the dataset if it was trained with the weak labels and then evaluate the model performance after the AED process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer from sklearn for converting text data into TF-IDF features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.classification import (\n",
    "    train_and_evaluate_classification_models,\n",
    "    print_classification_metrics,\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    strip_accents=\"unicode\",\n",
    "    lowercase=True,\n",
    "    max_features=1000,\n",
    "    min_df=3,\n",
    ")\n",
    "\n",
    "X_train = tfidf.fit_transform(df_train[\"text\"])\n",
    "y_train = df_train[\"label_majority_vote\"]\n",
    "\n",
    "X_dev = tfidf.transform(df_dev[\"text\"])\n",
    "y_dev = df_dev[\"label\"]\n",
    "\n",
    "X_test = tfidf.transform(df_test[\"text\"])\n",
    "y_test = df_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Calibrated-LSVC - F1: 0.9766 - Balanced Accuracy: 0.9704 - Accuracy: 0.9766 - Matthews Correlation Coefficient: 0.9410 - Elapsed time: 17.61s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     27604\n",
      "           1       0.98      0.98      0.98     73857\n",
      "\n",
      "    accuracy                           0.98    101461\n",
      "   macro avg       0.97      0.97      0.97    101461\n",
      "weighted avg       0.98      0.98      0.98    101461\n",
      "\n",
      "[[26405  1199]\n",
      " [ 1171 72686]]\n",
      "******************** \n",
      "\n",
      "Model: Logistic Regression - F1: 0.9661 - Balanced Accuracy: 0.9702 - Accuracy: 0.9661 - Matthews Correlation Coefficient: 0.9180 - Elapsed time: 2.76s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94     27604\n",
      "           1       0.99      0.96      0.98     73857\n",
      "\n",
      "    accuracy                           0.97    101461\n",
      "   macro avg       0.95      0.97      0.96    101461\n",
      "weighted avg       0.97      0.97      0.97    101461\n",
      "\n",
      "[[27024   580]\n",
      " [ 2857 71000]]\n",
      "******************** \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/imd3011-datacentric-ai-ZY2qswVr-py3.12/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning:\n",
      "\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest - F1: 0.9743 - Balanced Accuracy: 0.9642 - Accuracy: 0.9743 - Matthews Correlation Coefficient: 0.9348 - Elapsed time: 102.92s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95     27604\n",
      "           1       0.98      0.99      0.98     73857\n",
      "\n",
      "    accuracy                           0.97    101461\n",
      "   macro avg       0.97      0.96      0.97    101461\n",
      "weighted avg       0.97      0.97      0.97    101461\n",
      "\n",
      "[[26004  1600]\n",
      " [ 1008 72849]]\n",
      "******************** \n",
      "\n",
      "Model: XGBoost - F1: 0.9786 - Balanced Accuracy: 0.9720 - Accuracy: 0.9786 - Matthews Correlation Coefficient: 0.9459 - Elapsed time: 65.37s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     27604\n",
      "           1       0.98      0.99      0.99     73857\n",
      "\n",
      "    accuracy                           0.98    101461\n",
      "   macro avg       0.97      0.97      0.97    101461\n",
      "weighted avg       0.98      0.98      0.98    101461\n",
      "\n",
      "[[26431  1173]\n",
      " [  999 72858]]\n",
      "******************** \n",
      "\n",
      "Could not generate calibration plot for SGD: This 'SGDClassifier' has no attribute 'predict_proba'\n",
      "Model: SGD - F1: 0.9598 - Balanced Accuracy: 0.9665 - Accuracy: 0.9598 - Matthews Correlation Coefficient: 0.9044 - Elapsed time: 1.20s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93     27604\n",
      "           1       0.99      0.95      0.97     73857\n",
      "\n",
      "    accuracy                           0.96    101461\n",
      "   macro avg       0.94      0.97      0.95    101461\n",
      "weighted avg       0.96      0.96      0.96    101461\n",
      "\n",
      "[[27084   520]\n",
      " [ 3554 70303]]\n",
      "******************** \n",
      "\n",
      "Model: Naive Bayes - F1: 0.9473 - Balanced Accuracy: 0.9403 - Accuracy: 0.9473 - Matthews Correlation Coefficient: 0.8690 - Elapsed time: 1.68s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91     27604\n",
      "           1       0.97      0.96      0.96     73857\n",
      "\n",
      "    accuracy                           0.95    101461\n",
      "   macro avg       0.93      0.94      0.93    101461\n",
      "weighted avg       0.95      0.95      0.95    101461\n",
      "\n",
      "[[25531  2073]\n",
      " [ 3277 70580]]\n",
      "******************** \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/imd3011-datacentric-ai-ZY2qswVr-py3.12/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning:\n",
      "\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: K-Nearest Neighbors - F1: 0.8400 - Balanced Accuracy: 0.7182 - Accuracy: 0.8400 - Matthews Correlation Coefficient: 0.5713 - Elapsed time: 181.45s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.45      0.61     27604\n",
      "           1       0.83      0.99      0.90     73857\n",
      "\n",
      "    accuracy                           0.84    101461\n",
      "   macro avg       0.87      0.72      0.75    101461\n",
      "weighted avg       0.85      0.84      0.82    101461\n",
      "\n",
      "[[12447 15157]\n",
      " [ 1077 72780]]\n",
      "******************** \n",
      "\n",
      "Model: Decision Tree - F1: 0.9537 - Balanced Accuracy: 0.9431 - Accuracy: 0.9537 - Matthews Correlation Coefficient: 0.8834 - Elapsed time: 324.53s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92     27604\n",
      "           1       0.97      0.97      0.97     73857\n",
      "\n",
      "    accuracy                           0.95    101461\n",
      "   macro avg       0.94      0.94      0.94    101461\n",
      "weighted avg       0.95      0.95      0.95    101461\n",
      "\n",
      "[[25389  2215]\n",
      " [ 2484 71373]]\n",
      "******************** \n",
      "\n",
      "Model: Extra Trees - F1: 0.9768 - Balanced Accuracy: 0.9693 - Accuracy: 0.9768 - Matthews Correlation Coefficient: 0.9413 - Elapsed time: 159.61s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96     27604\n",
      "           1       0.98      0.99      0.98     73857\n",
      "\n",
      "    accuracy                           0.98    101461\n",
      "   macro avg       0.97      0.97      0.97    101461\n",
      "weighted avg       0.98      0.98      0.98    101461\n",
      "\n",
      "[[26307  1297]\n",
      " [ 1058 72799]]\n",
      "******************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_results, classification_reports, calibration_plot = (\n",
    "    train_and_evaluate_classification_models(X_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "F1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Balanced Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Matthews Correlation Coefficient",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Elapsed Time",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Confusion Matrix",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Classification Report",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ff305385-a701-4d0b-ab99-fd3a941ba25a",
       "rows": [
        [
         "3",
         "XGBoost",
         "0.9785927597796198",
         "0.9719900100896597",
         "0.9785927597796198",
         "0.9458558004969122",
         "65.36631417274475",
         "[[26431  1173]\n [  999 72858]]",
         "              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96     27604\n           1       0.98      0.99      0.99     73857\n\n    accuracy                           0.98    101461\n   macro avg       0.97      0.97      0.97    101461\nweighted avg       0.98      0.98      0.98    101461\n"
        ],
        [
         "8",
         "Extra Trees",
         "0.9767891110870186",
         "0.9693445382909658",
         "0.9767891110870186",
         "0.9412576022527284",
         "159.61051392555237",
         "[[26307  1297]\n [ 1058 72799]]",
         "              precision    recall  f1-score   support\n\n           0       0.96      0.95      0.96     27604\n           1       0.98      0.99      0.98     73857\n\n    accuracy                           0.98    101461\n   macro avg       0.97      0.97      0.97    101461\nweighted avg       0.98      0.98      0.98    101461\n"
        ],
        [
         "0",
         "Calibrated-LSVC",
         "0.9766412710302481",
         "0.9703546515397091",
         "0.9766412710302481",
         "0.9410084121765575",
         "17.6076762676239",
         "[[26405  1199]\n [ 1171 72686]]",
         "              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96     27604\n           1       0.98      0.98      0.98     73857\n\n    accuracy                           0.98    101461\n   macro avg       0.97      0.97      0.97    101461\nweighted avg       0.98      0.98      0.98    101461\n"
        ],
        [
         "2",
         "Random Forest",
         "0.9742955421294882",
         "0.9641946952177176",
         "0.9742955421294882",
         "0.9347687921322199",
         "102.9247989654541",
         "[[26004  1600]\n [ 1008 72849]]",
         "              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95     27604\n           1       0.98      0.99      0.98     73857\n\n    accuracy                           0.97    101461\n   macro avg       0.97      0.96      0.97    101461\nweighted avg       0.97      0.97      0.97    101461\n"
        ],
        [
         "1",
         "Logistic Regression",
         "0.9661249149919674",
         "0.9701528461310631",
         "0.9661249149919674",
         "0.9180313141334473",
         "2.7616326808929443",
         "[[27024   580]\n [ 2857 71000]]",
         "              precision    recall  f1-score   support\n\n           0       0.90      0.98      0.94     27604\n           1       0.99      0.96      0.98     73857\n\n    accuracy                           0.97    101461\n   macro avg       0.95      0.97      0.96    101461\nweighted avg       0.97      0.97      0.97    101461\n"
        ],
        [
         "4",
         "SGD",
         "0.95984664058111",
         "0.9665210673534785",
         "0.95984664058111",
         "0.9044107829742621",
         "1.2049269676208496",
         "[[27084   520]\n [ 3554 70303]]",
         "              precision    recall  f1-score   support\n\n           0       0.88      0.98      0.93     27604\n           1       0.99      0.95      0.97     73857\n\n    accuracy                           0.96    101461\n   macro avg       0.94      0.97      0.95    101461\nweighted avg       0.96      0.96      0.96    101461\n"
        ],
        [
         "7",
         "Decision Tree",
         "0.9536866382156691",
         "0.9430627229341783",
         "0.9536866382156691",
         "0.8834494232135564",
         "324.5323483943939",
         "[[25389  2215]\n [ 2484 71373]]",
         "              precision    recall  f1-score   support\n\n           0       0.91      0.92      0.92     27604\n           1       0.97      0.97      0.97     73857\n\n    accuracy                           0.95    101461\n   macro avg       0.94      0.94      0.94    101461\nweighted avg       0.95      0.95      0.95    101461\n"
        ],
        [
         "5",
         "Naive Bayes",
         "0.9472703797518258",
         "0.9402663315979924",
         "0.9472703797518258",
         "0.8690484341678048",
         "1.6781938076019287",
         "[[25531  2073]\n [ 3277 70580]]",
         "              precision    recall  f1-score   support\n\n           0       0.89      0.92      0.91     27604\n           1       0.97      0.96      0.96     73857\n\n    accuracy                           0.95    101461\n   macro avg       0.93      0.94      0.93    101461\nweighted avg       0.95      0.95      0.95    101461\n"
        ],
        [
         "6",
         "K-Nearest Neighbors",
         "0.8399976345590917",
         "0.7181653389689001",
         "0.8399976345590917",
         "0.5712933725881209",
         "181.45147609710693",
         "[[12447 15157]\n [ 1077 72780]]",
         "              precision    recall  f1-score   support\n\n           0       0.92      0.45      0.61     27604\n           1       0.83      0.99      0.90     73857\n\n    accuracy                           0.84    101461\n   macro avg       0.87      0.72      0.75    101461\nweighted avg       0.85      0.84      0.82    101461\n"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Matthews Correlation Coefficient</th>\n",
       "      <th>Elapsed Time</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.978593</td>\n",
       "      <td>0.971990</td>\n",
       "      <td>0.978593</td>\n",
       "      <td>0.945856</td>\n",
       "      <td>65.366314</td>\n",
       "      <td>[[26431  1173]\\n [  999 72858]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.976789</td>\n",
       "      <td>0.969345</td>\n",
       "      <td>0.976789</td>\n",
       "      <td>0.941258</td>\n",
       "      <td>159.610514</td>\n",
       "      <td>[[26307  1297]\\n [ 1058 72799]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Calibrated-LSVC</td>\n",
       "      <td>0.976641</td>\n",
       "      <td>0.970355</td>\n",
       "      <td>0.976641</td>\n",
       "      <td>0.941008</td>\n",
       "      <td>17.607676</td>\n",
       "      <td>[[26405  1199]\\n [ 1171 72686]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.974296</td>\n",
       "      <td>0.964195</td>\n",
       "      <td>0.974296</td>\n",
       "      <td>0.934769</td>\n",
       "      <td>102.924799</td>\n",
       "      <td>[[26004  1600]\\n [ 1008 72849]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.966125</td>\n",
       "      <td>0.970153</td>\n",
       "      <td>0.966125</td>\n",
       "      <td>0.918031</td>\n",
       "      <td>2.761633</td>\n",
       "      <td>[[27024   580]\\n [ 2857 71000]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.959847</td>\n",
       "      <td>0.966521</td>\n",
       "      <td>0.959847</td>\n",
       "      <td>0.904411</td>\n",
       "      <td>1.204927</td>\n",
       "      <td>[[27084   520]\\n [ 3554 70303]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.953687</td>\n",
       "      <td>0.943063</td>\n",
       "      <td>0.953687</td>\n",
       "      <td>0.883449</td>\n",
       "      <td>324.532348</td>\n",
       "      <td>[[25389  2215]\\n [ 2484 71373]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.947270</td>\n",
       "      <td>0.940266</td>\n",
       "      <td>0.947270</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>1.678194</td>\n",
       "      <td>[[25531  2073]\\n [ 3277 70580]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.839998</td>\n",
       "      <td>0.718165</td>\n",
       "      <td>0.839998</td>\n",
       "      <td>0.571293</td>\n",
       "      <td>181.451476</td>\n",
       "      <td>[[12447 15157]\\n [ 1077 72780]]</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model        F1  Balanced Accuracy  Accuracy  \\\n",
       "3              XGBoost  0.978593           0.971990  0.978593   \n",
       "8          Extra Trees  0.976789           0.969345  0.976789   \n",
       "0      Calibrated-LSVC  0.976641           0.970355  0.976641   \n",
       "2        Random Forest  0.974296           0.964195  0.974296   \n",
       "1  Logistic Regression  0.966125           0.970153  0.966125   \n",
       "4                  SGD  0.959847           0.966521  0.959847   \n",
       "7        Decision Tree  0.953687           0.943063  0.953687   \n",
       "5          Naive Bayes  0.947270           0.940266  0.947270   \n",
       "6  K-Nearest Neighbors  0.839998           0.718165  0.839998   \n",
       "\n",
       "   Matthews Correlation Coefficient  Elapsed Time  \\\n",
       "3                          0.945856     65.366314   \n",
       "8                          0.941258    159.610514   \n",
       "0                          0.941008     17.607676   \n",
       "2                          0.934769    102.924799   \n",
       "1                          0.918031      2.761633   \n",
       "4                          0.904411      1.204927   \n",
       "7                          0.883449    324.532348   \n",
       "5                          0.869048      1.678194   \n",
       "6                          0.571293    181.451476   \n",
       "\n",
       "                  Confusion Matrix  \\\n",
       "3  [[26431  1173]\\n [  999 72858]]   \n",
       "8  [[26307  1297]\\n [ 1058 72799]]   \n",
       "0  [[26405  1199]\\n [ 1171 72686]]   \n",
       "2  [[26004  1600]\\n [ 1008 72849]]   \n",
       "1  [[27024   580]\\n [ 2857 71000]]   \n",
       "4  [[27084   520]\\n [ 3554 70303]]   \n",
       "7  [[25389  2215]\\n [ 2484 71373]]   \n",
       "5  [[25531  2073]\\n [ 3277 70580]]   \n",
       "6  [[12447 15157]\\n [ 1077 72780]]   \n",
       "\n",
       "                               Classification Report  \n",
       "3                precision    recall  f1-score   ...  \n",
       "8                precision    recall  f1-score   ...  \n",
       "0                precision    recall  f1-score   ...  \n",
       "2                precision    recall  f1-score   ...  \n",
       "1                precision    recall  f1-score   ...  \n",
       "4                precision    recall  f1-score   ...  \n",
       "7                precision    recall  f1-score   ...  \n",
       "5                precision    recall  f1-score   ...  \n",
       "6                precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=\"Matthews Correlation Coefficient\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                                   Score\n",
      "=============================================\n",
      "Accuracy Score:                        0.95679\n",
      "Balanced Accuracy Score:               0.94548\n",
      "F1 Score (weighted):                   0.95664\n",
      "Cohen Kappa Score:                     0.89684\n",
      "Matthews Correlation Coefficient:      0.89696\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      7871\n",
      "           1       0.96      0.97      0.97     18187\n",
      "\n",
      "    accuracy                           0.96     26058\n",
      "   macro avg       0.95      0.95      0.95     26058\n",
      "weighted avg       0.96      0.96      0.96     26058\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "Class 0 has 654 false negatives and 472 false positives.\n",
      "Class 1 has 472 false negatives and 654 false positives.\n",
      "The total number of errors is 1126 out of 26058 samples (error rate: 0.0432).\n",
      "Predicted      0       1     All\n",
      "True                            \n",
      "0          7,217     654   7,871\n",
      "1            472  17,715  18,187\n",
      "All        7,689  18,369  26,058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Initialize a Logistic Regression model with balanced class weights\n",
    "# 'random_state' ensures reproducibility, 'n_jobs=-1' uses all available processors\n",
    "model_lr = LogisticRegression(random_state=271828, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "# Fit the Logistic Regression model on the training data\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "print_classification_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob_pos = model_lr.predict_proba(X_test)[:, 1]\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_test, prob_pos, n_bins=10\n",
    ")\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "plt.xlabel(\"Mean predicted value\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retagging with End-Model Predictions\n",
    "\n",
    "Retagging uses predictions from a model (often trained using cross-validation). We then compare these predictions with the existing labels, flagging discrepancies which may indicate mislabeling or inconsistencies.\n",
    "\n",
    "Retagging may improve label quality by:\n",
    "\n",
    "- **Reducing Noise:** Updating labels based on reliable predictions helps reveal the true patterns in the data.\n",
    "- **Improving Model Performance:** An enhanced dataset leads to better training for downstream models.\n",
    "- **Efficiency:** It offers a flexible approach when manual re-annotation for large datasets is impractical.\n",
    "\n",
    "> **Analogy:** Think of retagging like a spellchecker reviewing a document. The document (dataset) may have typos (label errors), and the spellchecker (model) suggests corrections. Reviewing these suggestions improves overall accuracy, much like retagging improves the quality of training data.\n",
    "\n",
    "The process can be summarized as correcting the dataset so that it more accurately represents the fundamental class structure, leading to a refined model training process.\n",
    "\n",
    "\n",
    "### Steps to Apply Retagging\n",
    "\n",
    "To carry out retagging, consider the following steps:\n",
    "\n",
    "1. **Train the Model on a Subset:**  \n",
    " Divide your dataset into subsets. Train the model using one subset to ensure that predictions for the remaining data are out-of-sample.\n",
    "\n",
    "2. **Obtain Out-of-Sample Predictions:**  \n",
    " Use the trained model to generate predictions on the data that was not used during training.\n",
    "\n",
    "3. **Compare Predictions with Existing Labels:**  \n",
    " Identify instances where the model's predictions differ from the given labels. These differences signal potential annotation errors.\n",
    "\n",
    "4. **Flag and Review Discrepancies:**  \n",
    " Treat the identified differences as candidates for correction. A systematic review (or even an iterative re-evaluation) can help decide whether to update the label.  \n",
    "\n",
    "Mathematically, if $ \\tilde{y}_i $ is the original label and $\\hat{y}_i $ is the model's predicted probability for class membership, a flag might be raised when:\n",
    "\n",
    "$$\n",
    "|\\hat{y}_i - \\tilde{y}_i| > \\epsilon\n",
    "$$\n",
    "\n",
    "where $epsilon $ is a chosen threshold reflecting acceptable variance.\n",
    "\n",
    "\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- **Model Confidence:**  \n",
    " When updating a label, consider the prediction confidence. High-confidence predictions are more likely to indicate actual errors and are safer to use for retagging.\n",
    "\n",
    "- **Iterative Refinement:**  \n",
    " Retagging can be repeated. After updating labels, retrain the model and recompute predictions, which can further enhance the dataset quality.\n",
    "\n",
    "- **Threshold Setting:**  \n",
    " Define thresholds for flagging discrepancies. For instance, only consider changes where:\n",
    "\n",
    "  $$\n",
    "  \\hat{y}_i > p \\quad \\text{or} \\quad \\hat{y}_i <1-p\n",
    "  $$\n",
    "\n",
    "  with $ p $ as probability threshold (e.g., 0.9) to ensure adjustments are based on confident predictions.\n",
    "\n",
    "- **Domain Expertise:**  \n",
    " Engage domain experts to review critical discrepancies. Their insight is valuable in ensuring that the retagging process aligns with real-world interpretations of the data.\n",
    "\n",
    "- **Avoiding Overfitting:**  \n",
    " It is essential that the predictions used for retagging are out-of-sample. Cross-validation techniques are advisable, as they provide a more realistic model performance estimate across the entire dataset.\n",
    "\n",
    "> **Note:** Retagging should be viewed as complement to manual annotation rather than a complete replacement. While it can correct many mistakes, there remains a risk of introducing new errors if the model makes incorrect predictions.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- For comparisons with other techniques on improving weak labels, consider reading [this study](https://arxiv.org/abs/2206.02280).  \n",
    "- For a detailed description of the retagging approach and more background, refer to the original [Retag Paper](https://aclanthology.org/W00-1907/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "# Initialize stratified k-fold cross-validation with 20 splits\n",
    "# StratifiedKFold ensures that each fold has the same proportion of classes as the original dataset\n",
    "cross_validation = StratifiedKFold(n_splits=20, shuffle=True, random_state=271828)\n",
    "\n",
    "# Initialize a Logistic Regression model with balanced class weights\n",
    "# 'random_state' ensures reproducibility, 'n_jobs=-1' uses all available processors\n",
    "model_retag = LogisticRegression(\n",
    "    random_state=271828, n_jobs=-1, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "# Perform cross-validated predictions on the training data\n",
    "# 'method=\"predict\"' returns the predicted class labels for each fold\n",
    "# 'n_jobs=2' is a trick to use all available processors threads, as n_jobs in model_retag is already set to -1\n",
    "y_train_retag = cross_val_predict(\n",
    "    estimator=model_retag,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=cross_validation,\n",
    "    method=\"predict\",\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                                   Score\n",
      "=============================================\n",
      "Accuracy Score:                        0.95817\n",
      "Balanced Accuracy Score:               0.95267\n",
      "F1 Score (weighted):                   0.95826\n",
      "Cohen Kappa Score:                     0.90124\n",
      "Matthews Correlation Coefficient:      0.90130\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      7871\n",
      "           1       0.97      0.97      0.97     18187\n",
      "\n",
      "    accuracy                           0.96     26058\n",
      "   macro avg       0.95      0.95      0.95     26058\n",
      "weighted avg       0.96      0.96      0.96     26058\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "Class 0 has 482 false negatives and 608 false positives.\n",
      "Class 1 has 608 false negatives and 482 false positives.\n",
      "The total number of errors is 1090 out of 26058 samples (error rate: 0.0418).\n",
      "Predicted      0       1     All\n",
      "True                            \n",
      "0          7,389     482   7,871\n",
      "1            608  17,579  18,187\n",
      "All        7,997  18,061  26,058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a Logistic Regression model with balanced class weights\n",
    "# 'random_state' ensures reproducibility, 'n_jobs=-1' uses all available processors\n",
    "model_lr = LogisticRegression(random_state=271828, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "# Fit the Logistic Regression model on the training data\n",
    "# y_train_retag is the re-tagged training labels obtained from cross-validation\n",
    "model_lr.fit(X_train, y_train_retag)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "print_classification_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                                   Score\n",
      "=============================================\n",
      "Accuracy Score:                        0.95875\n",
      "Balanced Accuracy Score:               0.94984\n",
      "F1 Score (weighted):                   0.95869\n",
      "Cohen Kappa Score:                     0.90191\n",
      "Matthews Correlation Coefficient:      0.90193\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93      7871\n",
      "           1       0.97      0.97      0.97     18187\n",
      "\n",
      "    accuracy                           0.96     26058\n",
      "   macro avg       0.95      0.95      0.95     26058\n",
      "weighted avg       0.96      0.96      0.96     26058\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "Class 0 has 572 false negatives and 503 false positives.\n",
      "Class 1 has 503 false negatives and 572 false positives.\n",
      "The total number of errors is 1075 out of 26058 samples (error rate: 0.0413).\n",
      "Predicted      0       1     All\n",
      "True                            \n",
      "0          7,299     572   7,871\n",
      "1            503  17,684  18,187\n",
      "All        7,802  18,256  26,058\n"
     ]
    }
   ],
   "source": [
    "# Dropping indexes where y_train_retag is different from y_train\n",
    "# This step ensures that only the examples where the re-tagged labels match the original labels are kept\n",
    "keep_indexes = (y_train_retag == y_train).values\n",
    "X_train_retag = X_train[keep_indexes]\n",
    "y_train_retag = y_train[keep_indexes]\n",
    "\n",
    "# Initialize a Logistic Regression model with balanced class weights\n",
    "# 'random_state' ensures reproducibility, 'n_jobs=-1' uses all available processors\n",
    "model_lr = LogisticRegression(random_state=271828, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "# Fit the Logistic Regression model on the filtered training data\n",
    "model_lr.fit(X_train_retag, y_train_retag)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = model_lr.predict(X_test)\n",
    "\n",
    "print_classification_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "The table below summarizes the reduction in annotation errors achieved by applying retagging to the dataset. The retagging process uses model predictions to drop instances in the weakly annotated dataset, leading to a cleaner and more consistent set of training data.\n",
    "\n",
    "| Dataset     | Errors | Improvement    |\n",
    "|-------------|--------|----------------|\n",
    " WS          |1126   |                |\n",
    "| WS + Retag  |1075   | **4.5%**       |\n",
    "\n",
    "We can see hat the retagging process has reduced the total number of errors by approximately 4.5%, a significant enhancement given the straightforward nature of the approach.\n",
    "\n",
    "\n",
    "### Retagging in Traditional Supervised Learning\n",
    "\n",
    "While commonly associated with weak supervision, **retagging** can also be a valuable technique in traditional supervised learning. In this context, it involves using the predictions of an intermediate model to refine the original training labels, ultimately improving the quality of the dataset used to train the final model.\n",
    "\n",
    "This process can be particularly helpful in addressing several common issues present in real-world datasets:\n",
    "\n",
    "- **Annotation Errors:** Human annotators can make mistakes. An intermediate model can help identify and potentially correct these errors, leading to a more accurate gold standard.\n",
    "- **Label Inconsistencies:** Large datasets annotated by multiple individuals can suffer from inconsistencies in label application. Retagging can help standardize labels and improve consistency.\n",
    "- **Ambiguities:** Some data points may be inherently ambiguous or fall into gray areas within the labeling schema. Retagging can capitalize on the intermediate model's learned representations to potentially resolve these ambiguities more consistently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pervasiveness and Impact of Label Noise in Real-World Datasets\n",
    "\n",
    "Label noise refers to errors or inaccuracies the assigned labels within a dataset. Research by [Northcutt et al., 2021](http://arxiv.org/abs/2103.14749) highlights that label noise is common in both training and test sets, and it can significantly affect the performance of machine learning models. A method known as **Confident Learning** was used in the study to estimate the amount of mislabeled data and to assess its impact.\n",
    "\n",
    "### Prevalence of Label Noise in Popular Datasets\n",
    "\n",
    "The study examined several widely used datasets and found a range of noise levels. The table below summarizes key findings:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Dataset        | CL Guessed | MTurk Checked | Validated Errors | Estimated Errors | % Error |\n",
    "|----------------|------------|----------------|----------------|----------------|----------|\n",
    " MNIST          |100        |100 (100%)     |15               | â                |0.15%    |\n",
    "| CIFAR-10       |275        |275 (100%)     |54               | â                |0.54%    |\n",
    "| CIFAR-100      |2,235      |2,235 (100%)   |585              | â                |5.85%    |\n",
    "| Caltech-256    |4,643      |400 (8.6%)     |65               |754              |2.46%    |\n",
    "| ImageNet\\*     |5,440      |5,440 (100%)   |2,916            | â                |5.83%    |\n",
    "| QuickDraw      |6,825,383  |2,500 (0.04%)  |1,870            |5,105,386        |10.12%   |\n",
    "| 20news         |93         |93 (100%)      |82               | â                |1.11%    |\n",
    "| IMDB           |1,310      |1,310 (100%)   |725              | â                |2.90%    |\n",
    "| Amazon         |533,249    |1,000 (0.2%)   |732              |390,338          |3.90%    |\n",
    "| AudioSet       |307        |307 (100%)     |275              | â                |1.35%    |\n",
    "\n",
    "</div>\n",
    "\n",
    "*Notes:*\n",
    "- **CL Guessed:** Number of samples suspected to be mislabeled by the Confident Learning method.\n",
    "- **MTurk Checked:** Number of samples verified by human annotators using Amazon Mechanical Turk.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Even datasets viewed as highly reliable, such as MNIST, contain mislabeled samples.\n",
    "- The percentage of label noise varies sharply across different datasets, ranging from0.15% to over10%.\n",
    "- In larger datasets, the absolute number of label errors can be very high even if the percentage appears moderate.\n",
    "\n",
    "\n",
    "### Impact on Model Performance\n",
    "\n",
    "Label noise can significantly influence a modelâs behavior:\n",
    "\n",
    "- **Learning Incorrect Associations:** When incorrect labels are present, models may develop erroneous relationships between features and labels.\n",
    "- **Reduced Prediction Accuracy:** The presence of mislabeled examples can lower accuracy on both the training set and unseen data.\n",
    "- **Overfitting on Noise:** Models may start fitting the noise instead of learning meaningful patterns, which is particularly harmful when the training data is noisy.\n",
    "\n",
    "*Example:* If images of cats are mistakenly labeled as dogs, the model might learn features for dogs that do not represent the true characteristics of the class, leading to poor classification performance when encountering new images.\n",
    "\n",
    "### Repercussions of Label Noise in Test Sets\n",
    "\n",
    "Label noise affects test data and has several drawbacks:\n",
    "\n",
    "- **Skewed Performance Metrics:** Evaluation metrics like accuracy or F1 score become less reliable when the test set contains labeling errors.\n",
    "- **Faulty Model Comparisons:** Using noisy test data for model comparisons might result in favoring models that perform well on mislabeled samples rather than those that capture true basic patterns.\n",
    "- **Misleading Assessment:** Relying on test results with significant noise might lead to an inflated sense of the modelâs capabilities.\n",
    "\n",
    "*Analogy:* Consider an exam where some of the answer choices are incorrectly marked. A studentâs score measured against such an answer key would not accurately reflect their true mastery of the subject.\n",
    "\n",
    "### Common Misconceptions\n",
    "\n",
    "- **Significance of Minor Noise:** Even a small percentage of mislabeled data can have a large impact, especially in critical applications such as medical diagnosis.\n",
    "- **Model Robustness:** While some algorithms may exhibit strength to random noise, they can still be misled if the noise is non-random or systematic. Real-world label noise often follows non-random patterns.\n",
    "- **Cost of Data Cleaning:** Investing resources in improving data quality through cleaning and validation typically reduces overall effort later in the modeling process by decreasing the need for compensatory mechanisms in model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Takeaways\n",
    "\n",
    "- High-quality, accurately labeled data is essential for building effective machine learning models. Investing in techniques like AED and retagging can dramatically improve model reliability.\n",
    "\n",
    "- Recognizing that label noise can be uniform, systematic, or instance-dependent encourages the use of tailored noise mitigation strategies. A nuanced approach to noise allows for better design of cleaning and training pipelines.\n",
    "\n",
    "- Incorporating measures of model confidence and calibration into noise detection provides a quantitative basis for flagging potential errors. This helps automate the data refinement process and informs decisions about manual review.\n",
    "\n",
    "- Rather than solely relying on more complex models to overcome noise, improving the training data through methods such as retagging and confident learning can lead to better model performance and more informative evaluation metrics.\n",
    "\n",
    "- The process of identifying, correcting, and re-evaluating noisy labels should be viewed as iterative. Continually refining the dataset helps ensure that both training and test data accurately reflect the true basic patterns.\n",
    "\n",
    "- Even highly regarded datasets (e.g., MNIST, ImageNet) can suffer from label noise, which means that reliable noise-handling methods are vital for realistic, generalizable models. Correcting these errors not only boosts performance but also leads to more trustworthy model evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is Annotation Error Detection (AED) and what are its two main types of methods?\n",
    "\n",
    "2. What are the three primary types of label noise in machine learning?\n",
    "\n",
    "3. How does the label noise transition matrix help in understanding and addressing label noise?\n",
    "\n",
    "4. What is retagging and how does it improve dataset quality?\n",
    "\n",
    "5. What is Confident Learning and how does it relate to detecting mislabeled examples?\n",
    "\n",
    "6. How do aleatoric and epistemic uncertainty differ in the context of prediction errors?\n",
    "\n",
    "7. What percentage of errors was reduced by applying retagging to the weakly supervised dataset according to the notebook?\n",
    "\n",
    "8. What are the key sources of noisy labels in datasets?\n",
    "\n",
    "9. How do model-centric and data-centric methods differ in addressing label noise?\n",
    "\n",
    "10. What was the range of label error percentages found in popular datasets according to Northcutt et al., 2021?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell.`\n",
    "\n",
    "<!-- \n",
    "\n",
    "1. Annotation Error Detection (AED) is a process for automatically identifying mislabeled or inconsistent examples in datasets. It employs two types of methods: binary flaggers that simply mark errors (providing a binary decision on whether an instance is likely correct or erroneous) and scorers that assign confidence scores indicating error likelihood (providing a probability score for each instance).\n",
    "\n",
    "2. The three primary types of label noise are: Uniform (Symmetric) Noise where every mislabel is equally likely, Systematic (Asymmetric) Noise where some classes are more likely to be confused than others, and Instance-Dependent Noise where the probability of mislabeling varies with specific features of each instance.\n",
    "\n",
    "3. The label noise transition matrix quantifies the probability of an observed noisy label given the true label. It represents the conditional probability p(á»¹=i|y*=j) and helps visualize the extent and pattern of label noise in dataset labels. This matrix is foundational to methods such as Confident Learning that adjust for label noise during training.\n",
    "\n",
    "4. Retagging is a technique that uses predictions from a held-out (out-of-sample) model to identify and correct potential mislabels. By comparing model outputs with existing labels, it identifies discrepancies which may indicate annotation errors. This helps refine the training data, reducing annotation errors and potentially improving overall model performance.\n",
    "\n",
    "5. Confident Learning is a methodology for automatically identifying, quantifying, and correcting mislabeled examples in datasets. It's based on the principle of using model confidence to estimate the joint distribution of noisy and true labels. It connects various methods for detecting mislabeled examples by considering them as ways to probe trained machine learning models.\n",
    "\n",
    "6. Aleatoric uncertainty is due to innate randomness or noise present in the data and is considered irreducible because it originates from the data-generation process. Epistemic uncertainty comes from limitations in the model's understanding of the relationship between inputs and outputs, reflecting gaps in the model's knowledge that can be reduced by improving the model.\n",
    "\n",
    "7. According to the notebook, applying retagging to the weakly supervised dataset reduced the total number of errors by approximately 4.5%.\n",
    "\n",
    "8. Key sources of noisy labels include: Human Error (accidental mislabeling, fatigue, lack of domain expertise), Measurement Issues (faulty data collection tools, inconsistent approaches), Algorithmic Errors (error propagation from earlier models, biases in automated labeling), Data Corruption (technical glitches, malicious tampering), and Subjective Interpretations (ambiguity in labeling, cultural variations).\n",
    "\n",
    "9. Model-centric methods address label noise by modifying the training process within the loss function (such as using loss from another network, direct loss modification, or importance reweighting). Data-centric methods focus on improving the quality of the training set by identifying and correcting label errors, removing mislabeled examples, or assigning lower weights to noisy samples.\n",
    "\n",
    "10. According to Northcutt et al., 2021, the percentage of label errors in popular datasets ranged from 0.15% (in MNIST) to over 10% (10.12% in QuickDraw). -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imd3011-datacentric-ai-ZY2qswVr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
